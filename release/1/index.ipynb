{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning 101++ in Python\n",
    "\n",
    "by Pieter Buteneers (@pieterbuteneers) and Bart De Vylder from CoScale\n",
    "\n",
    "\n",
    "\n",
    "## 1.1 Jupyter Notebook\n",
    "\n",
    "Jupyter notebook is often used by data scientists who work in Python. It is loosely based on Mathematica and combines code, text and visual output in one page.\n",
    "\n",
    "Some relevant short cuts:\n",
    "* ```SHIFT + ENTER``` executes 1 block of code called a cell\n",
    "* Tab completion is omnipresent after the import of a package has been executed\n",
    "* ```SHIFT + TAB``` gives you extra information on what parameters a function takes\n",
    "* Repeating ```SHIFT + TAB``` multiple times gives you even more information\n",
    "\n",
    "To get used to these short cuts try them out on the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'Hello world!'\n",
    "print range(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Numpy arrays\n",
    "\n",
    "We'll be working with numpy arrays so here's a very short introduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# This is a two-dimensional numpy array\n",
    "arr = np.array([[1,2,3,4],[5,6,7,8]])\n",
    "print arr\n",
    "\n",
    "# The shape is a tuple describing the size of each dimension\n",
    "print \"shape=\" + str(arr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The numpy reshape method allows one to change the shape of an array, while keeping the underlying data.\n",
    "# One can leave one dimension unspecified by passing -1, it will be determined from the size of the data.\n",
    "\n",
    "print \"As 4x2 matrix\" \n",
    "print np.reshape(arr, (4,2))\n",
    "\n",
    "print \n",
    "print \"As 8x1 matrix\" \n",
    "print np.reshape(arr, (-1,1))\n",
    "\n",
    "print \n",
    "print \"As 2x2x2 array\" \n",
    "print np.reshape(arr, (2,2,-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic arithmetical operations on arrays of the same shape are done elementwise: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = np.array([1.,2.,3.])\n",
    "y = np.array([4.,5.,6.])\n",
    "\n",
    "print x + y\n",
    "print x - y\n",
    "print x * y\n",
    "print x / y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Parts to be implemented\n",
    "\n",
    "In cells like the following example you are expected to implement some code. The remainder of the tutorial won't work if you skip these.\n",
    "\n",
    "Sometimes assertions are added as a check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "a5a20333c39df6f7341aea2834b389a8",
     "grade": false,
     "grade_id": "example_impl",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "##### Implement this part of the code #####\n",
    "raise NotImplementedError()\n",
    "# three = ?\n",
    "assert three == 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Imports\n",
    "\n",
    "The imports below will allow you to do any exercise without having done the previous exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab\n",
    "pylab.rcParams['figure.figsize'] = (13.0, 8.0)\n",
    "%matplotlib inline\n",
    "\n",
    "import cPickle as pickle\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import sklearn\n",
    "import sklearn.linear_model\n",
    "import sklearn.preprocessing\n",
    "import sklearn.gaussian_process\n",
    "import sklearn.ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Linear Regression\n",
    "\n",
    "Linear Regression assumes a linear realationship between 2 variables. \n",
    "\n",
    "As an example we'll consider the historical page views of a web server and compare it to its CPU usage. We'll try to predict the CPU usage of the server based on the page views of the different pages. \n",
    "\n",
    "## 2.1 Data import and inspection\n",
    "\n",
    "Let's import the data and take a look at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We use cPickle because it is faster\n",
    "import cPickle as pickle\n",
    "\n",
    "cpu_usage, page_views, page_names, total_page_views = pickle.load(open('data/cpu_page_views.pickle'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(cpu_usage)\n",
    "plt.plot(total_page_views)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The green line on the plot above is the number of page views and the blue line is the CPU load that viewing this pages generates on the server.\n",
    "\n",
    "### 3.2 Simple linear regression\n",
    "\n",
    "First, we're going to work with the total page views on the server, and compare it to the CPU usage. We can make use of a [PyPlot's scatter plot](http://matplotlib.org/api/pyplot_api.html#matplotlib.pyplot.scatter) to understand the relation between the total page views and the CPU usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "e53421d00e53760e05f97afc2bde280d",
     "grade": false,
     "grade_id": "scatter",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(13,8))\n",
    "plt.xlabel(\"Total page views\")\n",
    "plt.ylabel(\"CPU usage\")\n",
    "##### Implement this part of the code #####\n",
    "raise NotImplementedError()\n",
    "# plt.scatter( ? , ? )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There clearly is a strong correlation between the page views and the CPU usage. Because of this correlation we can build a model to predict the CPU usage from the total page views. If we use a linear model we get a formula like the following:\n",
    "\n",
    "$$ \\text{cpu_usage} = c_0 + c_1 \\text{total_page_views} $$\n",
    "\n",
    "Since we don't know the exact values for $c_0$ and $c_1$ we will have to compute them. For that we'll make use of the [scikit-learn](http://scikit-learn.org/stable/) machine learning library for Python and use [least-squares linear regression](http://scikit-learn.org/stable/modules/linear_model.html#ordinary-least-squares)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn.linear_model\n",
    "simple_lin_model = sklearn.linear_model.LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to feed the data to the model to fit it. The [model.fit(X,y) method](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression.fit) in general takes a matrix X and vector y as arguments:\n",
    "```\n",
    "      X = [[x_11, x_12, x_13, ...],                  y = [y_1,\n",
    "           [x_21, x_22, x_23, ...],                       y_2,  \n",
    "           [x_31, x_32, x_33, ...],                       y_3,\n",
    "           ...]                                           ...]\n",
    "\n",
    "```\n",
    "\n",
    "and tries to find coefficients that allow to predict the `y_i`'s from the `x_ij`'s. In our case the matrix X will consist of only 1 column containing the total page views. Our `total_page_views` variable however, is still only a one-dimensional vector, so we need to [`np.reshape()`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.reshape.html) it into a two-dimensional array. Since there is only 1 feature the second dimension should be 1.\n",
    "\n",
    "Then we fit our model using the the total page views and cpu. The coefficients found are automatically stored in the ```simple_lin_model``` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "3e5c1bfa06cee203d34e50b82988eed9",
     "grade": false,
     "grade_id": "model_fit",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "##### Implement this part of the code #####\n",
    "raise NotImplementedError()\n",
    "# simple_lin_model.fit( ? , ? ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now inspect the coefficient $c_1$ and constant term (intercept) $c_0$ of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Coefficient = %s, constant term = %f\" % (str(simple_lin_model.coef_), simple_lin_model.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this means that each additional page view adds about 0.11% CPU load to the server and all the other processes running on the server consume on average 0.72% CPU.\n",
    "\n",
    "Once the model is trained we can use it to [```predict```](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression.predict) the outcome for a given input (or array of inputs). Note that the predict function requires a 2-dimensional array similar to the ```fit``` function.\n",
    "\n",
    "What is the expected CPU usage when we have 880 page views per second?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "b92469e0851651e6488a87a7e400f2f2",
     "grade": false,
     "grade_id": "predict_100",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "##### Implement this part of the code #####\n",
    "raise NotImplementedError()\n",
    "# simple_lin_model.predict( [[ ? ]] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we plot the linear model together with our data to verify it captures the relationship correctly (the predict method can accept the entire ```total_page_views``` array at once)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbgrader": {
     "grade": false,
     "locked": false,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(13,8))\n",
    "\n",
    "plt.scatter(total_page_views, cpu_usage,  color='black')\n",
    "plt.plot(total_page_views, simple_lin_model.predict(total_page_views.reshape((-1, 1))), color='blue', linewidth=3)\n",
    "\n",
    "plt.xlabel(\"Total page views\")\n",
    "plt.ylabel(\"CPU usage\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model can calculate the R2 [`score`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression.score) indicating how well the linear model captures the data. A score of 1 means there is perfect linear correlation and the model can fit the data perfectly, a score of 0 (or lower) means that there is no correlation at all (and it does not make sense to try to model it that way). The score method takes the same arguments as the fit method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbgrader": {
     "grade": false,
     "locked": false,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "simple_lin_model.score(total_page_views.reshape((-1, 1)), cpu_usage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Extrapolation\n",
    "\n",
    "Now let's repeat this experiment with different but similar data. We will try to predict what the CPU usage will be if there will be 8 page views (per second)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cpu_usage, total_page_views = pickle.load(open('data/cpu_page_views_2.pickle'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "f58fd296f0f144aee2fa81964be31e73",
     "grade": false,
     "grade_id": "qwerqwer",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "##### Implement this part of the code #####\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot what you have done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_page_views = np.concatenate((total_page_views, [[8]]))\n",
    "\n",
    "plt.figure(figsize=(13,8))\n",
    "\n",
    "plt.scatter(total_page_views, cpu_usage,  color='black')\n",
    "plt.plot(all_page_views, simple_lin_model.predict(all_page_views), color='blue', linewidth=3)\n",
    "plt.axvline(8, color='r')\n",
    "\n",
    "plt.xlabel(\"Total page views\")\n",
    "plt.ylabel(\"CPU usage\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is this what you would expect? Can you see what's wrong?\n",
    "\n",
    "Let's plot the time series again to get a different view at the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(cpu_usage)\n",
    "plt.plot(total_page_views)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The spikes of CPU usage are actually backups that run at night and they can be ignored. So repeat the exersize again but ignore these data points.\n",
    "\n",
    "Hint: The selection variable should contain 1s where there is no backup going on and 0s when the backup occurs. This is an easy shortcut to do a selection of specific data points in numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "504f05e2c0f2625cb479773ea408b191",
     "grade": false,
     "grade_id": "qwerqwe",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "##### Implement this part of the code #####\n",
    "raise NotImplementedError()\n",
    "# selection = ?\n",
    "\n",
    "simple_lin_model = sklearn.linear_model.LinearRegression()\n",
    "simple_lin_model.fit(total_page_views[selection], cpu_usage[selection])\n",
    "prediction = simple_lin_model.predict([[8]])\n",
    "\n",
    "print 'The predicted value is:', prediction\n",
    "\n",
    "all_page_views = np.concatenate((total_page_views, [[8]]))\n",
    "\n",
    "plt.figure(figsize=(13,8))\n",
    "\n",
    "plt.scatter(total_page_views, cpu_usage,  color='black')\n",
    "plt.plot(all_page_views, simple_lin_model.predict(all_page_views), color='blue', linewidth=3)\n",
    "plt.axvline(8, color='r')\n",
    "\n",
    "plt.xlabel(\"Total page views\")\n",
    "plt.ylabel(\"CPU usage\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "assert prediction > 23"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what you should have learned from the previous exercise is that you should always look at your data and/or write scripts to inspect your data. Additinally extrapolation does not always work because there are no training examples in that area.\n",
    "\n",
    "## 3. Multiple linear regression\n",
    "\n",
    "A server can host different pages and each of the page views will generate load on the CPU. This load will however not be the same for each page.\n",
    "\n",
    "Now let us consider the separate page views and build a linear model for that. The model we try to fit takes the form:\n",
    "\n",
    "$$\\text{cpu_usage} = c_0 + c_1 \\text{page_views}_1 + c_2 \\text{page_views}_2 + \\ldots + c_n \\text{page_views}_n$$\n",
    "\n",
    "where the $\\text{page_views}_i$'s correspond the our different pages:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cpu_usage, page_views, page_names, total_page_views = pickle.load(open('data/cpu_page_views.pickle'))\n",
    "\n",
    "print page_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start again by creating a [```LinearRegression```](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression) model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "multi_lin_model = sklearn.linear_model.LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we fit the model on the data, using `multi_lin_model.fit(X,y)`. In contrast to the case above our `page_views` variable already has the correct shape to pass as the X matrix: it has one column per page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "f4475a0b36d0d86ba6931daae36d8c18",
     "grade": false,
     "grade_id": "multi_lin_model_fit",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "##### Implement this part of the code #####\n",
    "raise NotImplementedError()\n",
    "# multi_lin_model.fit( ? , ? )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, given the coefficients calculated by the model, which capture the contribution of each page view to the total CPU usage, we can start to answer some interesting questions. For example, \n",
    "which page view causes most CPU usage, on a per visit basis? \n",
    "\n",
    "For this we can generate a table of page names with their coefficients in descending order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbgrader": {
     "grade": false,
     "locked": false,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Although we do not advice to use pandas, it has its uses for some tasks\n",
    "import pandas as pd\n",
    "\n",
    "# combine the page views and the output in a pandas data frame for easy printing\n",
    "result_table = pd.DataFrame(zip(page_names, multi_lin_model.coef_), columns=['Page', 'Coef'])\n",
    "\n",
    "# sort the results in descending order\n",
    "result_table = result_table.sort_values(by='Coef',ascending=False)\n",
    "\n",
    "# executing this as the last command returns a nice table\n",
    "result_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this table we see that 'resources/js/basket.js' consumes the most per CPU per view. It generates about 0.30% CPU load for each additional page view. 'products/science.html' on the other hand is much leaner and only consumes about 0.04% CPU per view.\n",
    "\n",
    "Now let us investigate the constant term again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'The other processes on the server consume %.2f%%' % multi_lin_model.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see this term is very similar to the result achieved in single linear regression, but it is not entirely the same. This means that these models are not perfect. However, they seem to be able to give a reliable estimate.\n",
    "\n",
    "## 4. Non-linear Regression\n",
    "\n",
    "Sometimes linear relations don't cut it anymore, so you might want a more complex method. There are 2 approaches to this:\n",
    "* Use a non-linear method (such as Neural Networks, Support Vector Machines, Random Forrests and Gaussian Processes)\n",
    "* Use non-linear features as pre-processing for a linear method\n",
    "\n",
    "Actually both methods are in essence identical and there is not always a clear distinction between the two. Since it is easier to understand what is going on, we will use the second approach in this section.\n",
    "\n",
    "Please note that it is very often not even necessary to use non-linear methods, since the linear methods can be extremely powerful on their own and they are quite often very stable and reliable (in contrast to non-linear methods).\n",
    "\n",
    "### 4.1. Fitting a sine function with linear regression\n",
    "\n",
    "Als an example task we will try to fit a sine function. We will use the [`np.sine()`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.sin.html) function to compute the sine of the elements in a numpy array.\n",
    "\n",
    "Let's first try this with linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "3462210b922c0b778d35f4fa4b2c32f1",
     "grade": false,
     "grade_id": "average_cpu_without",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "##### Implement this part of the code #####\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For training we will draw 10 samples of this function as our train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_train = np.linspace(0, 6, 10).reshape((-1, 1))\n",
    "y_train = np.sin(x_train)\n",
    "\n",
    "plt.scatter(x_train, y_train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Now let's try to fit this function with linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "f75002099c4d6b726d818440d0144bc4",
     "grade": false,
     "grade_id": "qwerq2",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "##### Implement this part of the code #####\n",
    "raise NotImplementedError()\n",
    "# model = ?\n",
    "# model.fit( ? )\n",
    "\n",
    "print 'The R2 score of this model is:', model.score(x_train, y_train)\n",
    "\n",
    "plt.scatter(x_train, y_train)\n",
    "plt.plot(x, model.predict(x))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see this fit is not optimal.\n",
    "\n",
    "### 4.2. Fitting a sine function using polynomial expansion\n",
    "\n",
    "As some of you might remember from math class is that you can create an approximation of any function using a polynomial function usint the [Taylor expansion](https://en.wikipedia.org/wiki/Taylor_series). So we will use that approach to learn a better fit.\n",
    "\n",
    "This means that as a machine learning engineer it is best to know your data. That way you can do some preprocessing to make it easier for your model to fit the data. \n",
    "\n",
    "In this case we will create what we call features using a [polynomial expansion](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html). If you set the degree to 3 it will generate data of the 1st, 2nd and 3rd order (including cross products)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sklearn.preprocessing\n",
    "\n",
    "pol_exp = sklearn.preprocessing.PolynomialFeatures(degree=3)\n",
    "\n",
    "model = sklearn.linear_model.LinearRegression()\n",
    "model.fit(pol_exp.fit_transform(x_train), y_train)\n",
    "print 'The R2 score of this model is:', model.score(pol_exp.fit_transform(x_train), y_train)\n",
    "\n",
    "plt.scatter(x_train, y_train)\n",
    "x = np.arange(0,6, 0.01).reshape((-1, 1))\n",
    "plt.plot(x, model.predict(pol_exp.fit_transform(x)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The more relevant these features are the better your model can fit the data.\n",
    "\n",
    "Now play with the degree of the polynomal expansion function below to create better features. Search for the optimal degree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "8ffc426906915f4bf0484c3c46d4be3e",
     "grade": false,
     "grade_id": "qwerq",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "##### Implement this part of the code #####\n",
    "raise NotImplementedError()\n",
    "# pol_exp = sklearn.preprocessing.PolynomialFeatures(degree= ? )\n",
    "\n",
    "model = sklearn.linear_model.LinearRegression()\n",
    "model.fit(pol_exp.fit_transform(x_train), y_train)\n",
    "\n",
    "train_score = model.score(pol_exp.fit_transform(x_train), y_train)\n",
    "print 'The R2 score of this model is:', train_score\n",
    "\n",
    "plt.scatter(x_train, y_train)\n",
    "x = np.arange(0,6, 0.01).reshape((-1, 1))\n",
    "plt.plot(x, model.predict(pol_exp.fit_transform(x)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's test this on new and unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "00a2e9333b7fe1c5be484f135b65c2c5",
     "grade": false,
     "grade_id": "qwer",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "assert train_score > .99999\n",
    "\n",
    "x_test = 0.5 + np.arange(6).reshape((-1, 1))\n",
    "\n",
    "plt.scatter(x_train, y_train)\n",
    "plt.scatter(x_test, np.sin(x_test), color='r')\n",
    "x = np.arange(0, 6, 0.01).reshape((-1, 1))\n",
    "plt.plot(x, model.predict(pol_exp.fit_transform(x)))\n",
    "plt.show()\n",
    "\n",
    "##### Implement this part of the code #####\n",
    "raise NotImplementedError()\n",
    "# score = model.score( ? )\n",
    "\n",
    "print 'The R2 score of the model on the test set is:', test_score\n",
    "\n",
    "assert test_score > 0.99"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything is correct your score is very close to 1. Which means that we have built a model that can fit this data (almost) perfectly.\n",
    "\n",
    "### 4.3. Add noise to the equation\n",
    "\n",
    "Sadly all the data that we measure or gather doesn'thave the mathematical precision of the data we used here. Quite often our measurements contain noise.\n",
    "\n",
    "So let us repeat this process for data with more noise. Similarly as above, you have to choose the optimal degree of the polynomials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_train_noise = x_train + np.random.randn(len(x_train)).reshape((-1, 1)) / 10\n",
    "\n",
    "pol_exp = sklearn.preprocessing.PolynomialFeatures(degree=12)\n",
    "\n",
    "model = sklearn.linear_model.LinearRegression()\n",
    "model.fit(pol_exp.fit_transform(x_train_noise), y_train)\n",
    "print 'The R2 score of this method on the train set is', model.score(pol_exp.fit_transform(x_train_noise), y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see what this results to in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'The R2 score of the model on the test set is:', model.score(pol_exp.fit_transform(x_test), np.sin(x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can clearly see, this result is not that good. Why do you think this is?\n",
    "\n",
    "Now plot the result to see the function you created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(x_train_noise, y_train)\n",
    "x = np.arange(0,6, 0.01).reshape((-1, 1))\n",
    "plt.plot(x, model.predict(pol_exp.fit_transform(x)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is this what you expect?\n",
    "\n",
    "Now repeat the process below a couple of times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_train_noise = x_train + np.random.randn(len(x_train)).reshape((-1, 1)) / 10\n",
    "\n",
    "pol_exp = sklearn.preprocessing.PolynomialFeatures(degree=9)\n",
    "\n",
    "model = sklearn.linear_model.LinearRegression()\n",
    "model.fit(pol_exp.fit_transform(x_train_noise), y_train)\n",
    "print 'The score of this method is', model.score(pol_exp.fit_transform(x_train_noise), y_train)\n",
    "\n",
    "plt.scatter(x_train_noise, y_train)\n",
    "x = np.arange(0,6, 0.01).reshape((-1, 1))\n",
    "plt.plot(x, model.predict(pol_exp.fit_transform(x)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What did you observe? And what is the method learning? And how can you avoid this?\n",
    "\n",
    "Try to figure out a solution for this problem without changing the noise level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "59a6fde7271151e0a58b9e5d477d5aea",
     "grade": false,
     "grade_id": "qwe",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "x_train_noise = x_train + np.random.randn(len(x_train)).reshape((-1, 1)) / 10\n",
    "\n",
    "##### Implement this part of the code #####\n",
    "raise NotImplementedError()\n",
    "# pol_exp = ?\n",
    "# model = ?\n",
    "# model.fit( ? )\n",
    "\n",
    "print 'The score of this method is on the train set is:', model.score(pol_exp.fit_transform(x_train_noise), y_train)\n",
    "\n",
    "plt.scatter(x_train_noise, y_train)\n",
    "x = np.arange(0,6, 0.01).reshape((-1, 1))\n",
    "plt.plot(x, model.predict(pol_exp.fit_transform(x)))\n",
    "plt.show()\n",
    "\n",
    "test_score = model.score(pol_exp.fit_transform(x_test), np.sin(x_test))\n",
    "print 'The score of the model on the test set is:', test_score\n",
    "\n",
    "assert test_score > 0.97"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check your solution a couple of times to make sure your solution works for different noise samples.\n",
    "\n",
    "## 5. Over-fitting and Cross-Validation\n",
    "\n",
    "What you have experienced above is called over-fitting and happens when your model learns the noise that is inherrent in the data.\n",
    "\n",
    "This problem was caused because there were to many parameters in the model. So the model was too advanced so that it became capable of learning the noise in the data by hart. Reducing the number of parameters solves this problem. But how do you know how many parameters is optimal?\n",
    "\n",
    "(Another way to solve this problem is to use more data. Because if there are more data points in the data and if there is more noise, your model isn't able to learn all that noise anymore and you get a better result. Since it usually is not possible to gather more data we will not take this approach.)\n",
    "\n",
    "In the exercise above you had to set the number of polynomial functions to get a better result, but how can you estimate this in a reliable way without manually selection the optimal parameters?\n",
    "\n",
    "### 5.1. Validation set\n",
    "\n",
    "A Common way to solve this problem is through the use of a validation set. This means that you use a subset of the training data to train your model on, and another subset of the training data to validate your parameters. Based on the score of your model on this validation set you can select the best optimal parameter.\n",
    "\n",
    "So use this approach to select the best number of polynomials for the noisy sine function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "f3ee1bc186cb163af2772fb92e428750",
     "grade": false,
     "grade_id": "qw",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "x_train_noise = x_train + np.random.randn(len(x_train)).reshape((-1, 1)) / 10\n",
    "\n",
    "# randomly shuffle the data to get a nice validation set\n",
    "permutation = np.random.permutation(10)\n",
    "\n",
    "# use 70% of the data for training and 30% for testing. \n",
    "# these numbers are arbitrary and depend on the avialability of data\n",
    "x_train_i = x_train_noise[permutation[ : 7], :]\n",
    "y_train_i = y_train[permutation[ : 7]]\n",
    "x_val_i = x_train_noise[permutation[7 : ], :]\n",
    "y_val_i = y_train[permutation[7 : ]]\n",
    "\n",
    "##### Implement this part of the code #####\n",
    "raise NotImplementedError()\n",
    "# pol_exp = sklearn.preprocessing.PolynomialFeatures(degree= ? )\n",
    "\n",
    "model = sklearn.linear_model.LinearRegression()\n",
    "model.fit(pol_exp.fit_transform(x_train_i), y_train_i)\n",
    "\n",
    "##### Implement this part of the code #####\n",
    "raise NotImplementedError()\n",
    "# print 'The R2 score of this model on the train set is:', model.score( ? )\n",
    "# print 'The R2 score of this model on the validation set is:', model.score( ? )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat that a couple of times for different noise samples.\n",
    "\n",
    "Now test this result on the test set with the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert pol_exp.degree < 4\n",
    "\n",
    "plt.scatter(x_train_noise, y_train)\n",
    "plt.scatter(x_test, np.sin(x_test), color='r')\n",
    "x = np.arange(0,6, 0.01).reshape((-1, 1))\n",
    "plt.plot(x, model.predict(pol_exp.fit_transform(x)))\n",
    "plt.show()\n",
    "\n",
    "print 'The score of the model on the test set is:', model.score(pol_exp.fit_transform(x_test), np.sin(x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Cross-Validation\n",
    "\n",
    "To improve this procedure you can repeat the process above for different train and validation sets so that the optimal parameter is less dependent on the way the data was selected.\n",
    "\n",
    "Now let's adapt the code above to automate this.\n",
    "\n",
    "We will use the [mean squared error](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html) instead (smaller is better) of R2 because that is more stable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "d7c67d679375c2edd3c4cc6230676520",
     "grade": false,
     "grade_id": "q",
     "locked": false,
     "solution": true
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x_train_noise = x_train + np.random.randn(len(x_train)).reshape((-1, 1)) / 10\n",
    "\n",
    "##### Implement this part of the code #####\n",
    "raise NotImplementedError()\n",
    "# results = np.zeros( ? )\n",
    "\n",
    "# for i in range( ? ):\n",
    "    train_i = np.where(np.arange(10) != i)[0]\n",
    "    x_train_i = x_train_noise[train_i, :]\n",
    "    y_train_i = y_train[train_i]\n",
    "    x_val_i = x_train_noise[i:i+2, :]\n",
    "    y_val_i = y_train[i:i+2]\n",
    "\n",
    "##### Implement this part of the code #####\n",
    "raise NotImplementedError()\n",
    "    # for degree in range(?):\n",
    "        pol_exp = sklearn.preprocessing.PolynomialFeatures(degree=degree)\n",
    "\n",
    "        model = sklearn.linear_model.LinearRegression()\n",
    "        model.fit(pol_exp.fit_transform(x_train_i), y_train_i)\n",
    "        \n",
    "##### Implement this part of the code #####\n",
    "raise NotImplementedError()\n",
    "        # results[ ? ] = sklearn.metrics.mean_squared_error(model.predict(pol_exp.fit_transform(x_val_i)), y_val_i)\n",
    "\n",
    "##### Implement this part of the code #####\n",
    "raise NotImplementedError()\n",
    "# average the results over all validation sets\n",
    "# average_results = np.mean(results, axis= ? )\n",
    "# find the optmal degree\n",
    "# degree = np.argmin( ? ) + ?\n",
    "\n",
    "print 'The optimal degree for the polynomials is:', degree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's have a look at the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert degree > 2\n",
    "assert degree < 6\n",
    "\n",
    "pol_exp = sklearn.preprocessing.PolynomialFeatures(degree=degree)\n",
    "\n",
    "model = sklearn.linear_model.LinearRegression()\n",
    "model.fit(pol_exp.fit_transform(x_train_noise), y_train)\n",
    "print 'The score of this method is on the train set is:', model.score(pol_exp.fit_transform(x_train_noise), y_train)\n",
    "\n",
    "plt.scatter(x_train_noise, y_train)\n",
    "plt.scatter(x_test, np.sin(x_test), color='r')\n",
    "x = np.arange(0,6, 0.01).reshape((-1, 1))\n",
    "plt.plot(x, model.predict(pol_exp.fit_transform(x)))\n",
    "plt.show()\n",
    "\n",
    "print 'The score of the model on the test set is:', model.score(pol_exp.fit_transform(x_test), np.sin(x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Regularisation\n",
    "\n",
    "Since if you have to many parameters in your model, it learns the noise, there are techniques that have been developed to make an estimation of this noise. \n",
    "\n",
    "One of these methods is Ridge Regression. It has an additional parameter called the regularisation parameter. This parameter basically sets the standard deviation of the noise. What this does in practis is that it makes sure the weights of linear regression remain small.\n",
    "\n",
    "Since this is an additional parameter that needs to be set, this parameter needs to be corss-validated as well. Luckily sklearn developed a method that does this for us called [`sklearn.linear_model.RidgeCV()`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeCV.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "f129ad53bf00231c9d8a5105d75e90e8",
     "grade": false,
     "grade_id": "asdfasdf",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "x_train_noise = x_train + np.random.randn(len(x_train)).reshape((-1, 1)) / 10\n",
    "\n",
    "##### Implement this part of the code #####\n",
    "raise NotImplementedError()\n",
    "# pol_exp = sklearn.preprocessing.PolynomialFeatures(degree= ? )\n",
    "# model = sklearn.linear_model. ?\n",
    "\n",
    "\n",
    "model.fit(pol_exp.fit_transform(x_train_noise), y_train)\n",
    "print 'The R2 score of this method is on the train set is:', model.score(pol_exp.fit_transform(x_train_noise), y_train)\n",
    "\n",
    "plt.scatter(x_train_noise, y_train)\n",
    "plt.scatter(x_test, np.sin(x_test), color='r')\n",
    "x = np.arange(0,6, 0.01).reshape((-1, 1))\n",
    "plt.plot(x, model.predict(pol_exp.fit_transform(x)))\n",
    "plt.show()\n",
    "\n",
    "print 'The R2 score of the model on the test set is:', model.score(pol_exp.fit_transform(x_test), np.sin(x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above, the result of Ridge Regression is not as good as reducing the number of features in this example. However it works a lot better than without regularisation. So this means that it should be in your standard toolkit.\n",
    "\n",
    "The removel of the extra features can be automated using feature selection. A very short introduction to sklearn on the topic can be found [here](http://scikit-learn.org/stable/modules/feature_selection.html).\n",
    "\n",
    "Another method that is often used is [`sklearn.linear_model.LassoCV()`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoCV.html#sklearn.linear_model.LassoCV) which actually combines removal of features and estimation of the noise. It is however very dependant on the dataset which of the two methods performs best.\n",
    "\n",
    "Cross-validation should be applied to any parameter you set in your function and that without looking at the test set.\n",
    "\n",
    "Over-fitting is one of the biggest issues in machine learning and most of the research that is currently being done in machine learning is a search for techniques to avoid over-fitting. As a starting point we list a few of the techniques that you can use to avoid over-fitting:\n",
    "* Use more data\n",
    "* Artificially generate more data based on the orriginal data\n",
    "* Use a smaller model (with less parameters)\n",
    "* Use less features (and thus less parameters)\n",
    "* Use a regularisation parameter\n",
    "* Artificially add noise to your model\n",
    "* Only use linear models or make sure that the non-linearity in your model is closer to a linear function\n",
    "* Combine multiple models that each over-fit in their own way into what is called an ensemble\n",
    "\n",
    "### 5.4 Extrapolation\n",
    "\n",
    "Now let's extend the range of the optimal plot you achieved from -4 to 10. What do you see? Does it look like a sine function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_train_noise = x_train + np.random.randn(len(x_train)).reshape((-1, 1)) / 10\n",
    "\n",
    "pol_exp = sklearn.preprocessing.PolynomialFeatures(degree=4)\n",
    "\n",
    "model = sklearn.linear_model.RidgeCV()\n",
    "model.fit(pol_exp.fit_transform(x_train_noise), y_train)\n",
    "print 'The score of this method is on the train set is:', model.score(pol_exp.fit_transform(x_train_noise), y_train)\n",
    "\n",
    "plt.scatter(x_train_noise, y_train)\n",
    "plt.scatter(x_test, np.sin(x_test), color='r')\n",
    "x = np.arange(-4,10, 0.01).reshape((-1, 1))\n",
    "plt.plot(x, model.predict(pol_exp.fit_transform(x)))\n",
    "plt.show()\n",
    "\n",
    "print 'The score of the model on the test set is:', model.score(pol_exp.fit_transform(x_test), np.sin(x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the extrapolation results for non-linear regression are even worse than for those of linear regression. This is because models are only good in the are they have been trained. \n",
    "\n",
    "A possible way to be able to extrapolate and to use a non-linear method is to use forecasting techniques. This part is optional for those interested and going through the tutorial quite fast. Otherwise continue to the final part on classification in exercise 7.\n",
    "\n",
    "## 6. Forecasting (Optional)\n",
    "\n",
    "For the forecasting we are going to use page views data, very similar to the data used in the anomaly detection section. It is also page view data and contains 1 sample per hour. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_set = pickle.load(open('data/train_set_forecasting.pickle'))\n",
    "\n",
    "plt.figure(figsize=(20,4))\n",
    "plt.plot(train_set)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the graph above you can clearly see that there is a rising trend in the data.\n",
    "\n",
    "### 6.1 One-step ahead prediction\n",
    "\n",
    "This forecasting section will describe the one-step ahead prediction. This means in this case that we will only predict the next data point which is in this case the number of pageviews in the next hour.\n",
    "\n",
    "Now let's first build a model that tries to predict the next data point from the previous one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import sklearn.linear_model\n",
    "import sklearn.gaussian_process\n",
    "\n",
    "model = sklearn.linear_model.LinearRegression()\n",
    "\n",
    "# the input X contains all the data except the last data point\n",
    "X = train_set[ : -1].reshape((-1, 1)) # the reshape is necessary since sklearn requires a 2 dimensional array\n",
    "\n",
    "# the output y contains all the data except the first data point\n",
    "y = train_set[1 : ]\n",
    "\n",
    "# this code fits the model on the train data\n",
    "model.fit(X, y)\n",
    "\n",
    "# this score gives you how well it fits on the train set\n",
    "# higher is better and 1.0 is perfect\n",
    "print 'The R2 train score of the linear model is', model.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from the score above, the model is not perfect but it seems to get a relatively high score. Now let's make a prediction into the future and plot this.\n",
    "\n",
    "To predict the datapoint after that we will use the predicted data to make a new prediction. The code below shows how this works for this data set using the linear model you used earlier. Don't forget to fill out the missing code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "0d3070581eb28088e118c928fafdfe6c",
     "grade": false,
     "grade_id": "nof_predictions",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "nof_predictions = 100\n",
    "\n",
    "import copy\n",
    "# use the last data point as the first input for the predictions\n",
    "x_test = copy.deepcopy(train_set[-1]) # make a copy to avoid overwriting the training data\n",
    "\n",
    "prediction = []\n",
    "for i in range(nof_predictions):\n",
    "    # predict the next data point\n",
    "    y_test = model.predict([[x_test]])[0] # sklearn requires a 2 dimensional array and returns a one-dimensional one\n",
    "    \n",
    "    ##### Implement this part of the code #####\n",
    "    raise NotImplementedError()\n",
    "    # prediction.append( ? )\n",
    "    # x_test = ?\n",
    "\n",
    "prediction = np.array(prediction)\n",
    "\n",
    "plt.figure(figsize=(20,4))\n",
    "plt.plot(np.concatenate((train_set, prediction)), 'g')\n",
    "plt.plot(train_set, 'b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from the image above the model doesn't quite seem to fit the data well. Let's see how we can improve this.\n",
    "\n",
    "### 6.2 Multiple features\n",
    "\n",
    "If your model is not smart enough there is a simple trick in machine learning to make your model more intelligent (but also more complex). This is by adding more features.\n",
    "\n",
    "To make our model better we will use more than 1 sample from the past. To make your life easier there is a simple function below that will create a data set for you. The ```width``` parameter sets the number of hours in the past that will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_time_series_to_Xy(ts, width):\n",
    "    X, y = [], []\n",
    "    for i in range(len(ts) - width - 1):\n",
    "        X.append(ts[i : i + width])\n",
    "        y.append(ts[i + width])\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "width = 5\n",
    "X, y = convert_time_series_to_Xy(train_set, width)\n",
    "\n",
    "print X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from the print above both X and y contains 303 datapoints. For X you see that there are now 5 features which contain the pageviews from the 5 past hours.\n",
    "\n",
    "So let's have a look what the increase from 1 to 5 features results to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "width = 5\n",
    "X, y = convert_time_series_to_Xy(train_set, width)\n",
    "model = sklearn.linear_model.LinearRegression()\n",
    "model.fit(X,y)\n",
    "print 'The score of the linear model with width =', width, 'is', model.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now change the ```width``` parameter to see if you can get a better score.\n",
    "\n",
    "### 6.3 Over-fitting\n",
    "\n",
    "\n",
    "Now execute the code below to see the prediction of this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "# this is a helper function to make the predictions\n",
    "def predict(model, train_set, width, nof_points):\n",
    "    prediction = []\n",
    "    # create the input data set for the first predicted output\n",
    "    # copy the data to make sure the orriginal is not overwritten\n",
    "    x_test = copy.deepcopy(train_set[-width : ]) \n",
    "    for i in range(nof_points):\n",
    "        # predict only the next data point\n",
    "        prediction.append(model.predict(x_test.reshape((1, -1))))\n",
    "        # use the newly predicted data point as input for the next prediction\n",
    "        x_test[0 : -1] = x_test[1 : ]\n",
    "        x_test[-1] = prediction[-1]\n",
    "    return np.array(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nof_predictions = 200\n",
    "prediction = predict(model, train_set, width, nof_predictions)\n",
    "\n",
    "plt.figure(figsize=(20,4))\n",
    "plt.plot(np.concatenate((train_set, prediction[:,0])), 'g')\n",
    "plt.plot(train_set, 'b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see in the image above the prediction is not what you would expect from a perfect model. What happened is that the model learned the training data by heart without 'understanding' what the data is really about. This fenomenon is called over-fitting and will always occur if you make your model more complex.\n",
    "\n",
    "Now play with the width variable below to see if you can find a more sensible width."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "5ac9a93dd304f03fd89896b07621761e",
     "grade": false,
     "grade_id": "jkl",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "##### Implement this part of the code #####\n",
    "raise NotImplementedError()\n",
    "# width = ?\n",
    "\n",
    "X, y = convert_time_series_to_Xy(train_set, width)\n",
    "model = sklearn.linear_model.LinearRegression()\n",
    "model.fit(X,y)\n",
    "print 'The score of the linear model with width =', width, 'is', model.score(X, y)\n",
    "\n",
    "prediction = predict(model, train_set, width, 200)\n",
    "\n",
    "plt.figure(figsize=(20,4))\n",
    "plt.plot(np.concatenate((train_set, prediction[:,0])), 'g')\n",
    "plt.plot(train_set, 'b')\n",
    "plt.show()\n",
    "\n",
    "assert width > 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you will have noticed by now is that it is better to have a non-perfect score which will give you a much better outcome. Now try the same thing for the following models:\n",
    "* ```sklearn.linear_model.RidgeCV()```\n",
    "* ```sklearn.linear_model.LassoCV()```\n",
    "* ```sklearn.gaussian_process.GaussianProcess()```\n",
    "\n",
    "The first 2 models also estimate the noise that is present in the data to avoid overfitting. `RidgeCV()` will keep the weights that are found small, but it won't put them to zero. `LassoCV()` on the other hand will put several weights to 0. Execute ```model.coef_``` to see the actual coefficients that have been found.\n",
    "\n",
    "`GaussianProcess()` is a non-linear method. This makes this method a lot more complex and therefore it will need significantly less features to be able to learn the data by hart (and thus to over-fit). In many cases however this additional complexity allows to better understand the data (try `width = 20`). Additionally it has the advantage that it can estimate confidance intervals.\n",
    "\n",
    "### 6.4 Automation\n",
    "\n",
    "What we have done up to now is manually selecting the best outcome based on the test result. This can be considered cheating because you have just created a self-fulfilling prophecy. Additionally it is not only cheating it is also hard to find the exact `width` that gives the best result by just visually inspecting it. So we need a more objective approach to solve this.\n",
    "\n",
    "To automate this process you can use a validation set. In this case we will use the last 48 hours of the training set to validate the score and select the best parameter value. This means that we will have to use a subset of the training set to fit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "c43a8d7bb56d327a62ba9d4af0ab2ddc",
     "grade": false,
     "grade_id": "find_best_model",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "model_generators = [sklearn.linear_model.LinearRegression, sklearn.linear_model.RidgeCV,\n",
    "                    sklearn.linear_model.LassoCV, sklearn.gaussian_process.GaussianProcess]\n",
    "best_score = 0\n",
    "\n",
    "##### Implement this part of the code #####\n",
    "raise NotImplementedError()\n",
    "# for model_gen in ? :\n",
    "#    for width in range( ? , ? ): \n",
    "        X, y = convert_time_series_to_Xy(train_set, width)\n",
    "        # train the model on the first 48 hours\n",
    "        X_train, y_train = X[ : -48, :], y[ : -48]\n",
    "        # use the last 48 hours for validation\n",
    "        X_val, y_val = X[-48 : ], y[-48 : ]\n",
    "        \n",
    "        ##### Implement this part of the code #####\n",
    "        raise NotImplementedError()\n",
    "        # model = \n",
    "        \n",
    "        # there is a try except clause here because some models do not converge for some data\n",
    "        try:\n",
    "            ##### Implement this part of the code #####\n",
    "            raise NotImplementedError()\n",
    "            # model.fit( ? , ? )\n",
    "            # this_score = ?\n",
    "            \n",
    "            if this_score > best_score:\n",
    "                best_score = this_score\n",
    "                best_model_gen = model_gen\n",
    "                best_width = width\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "print best_model_gen().__class__, 'was selected as the best model with a width of', best_width, 'and a validation score of', best_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything is correct the LassoCV methods was selected.\n",
    "\n",
    "Now we are going to train this best model on all the data. In this way we use all the available data to build a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "115e77264f141df6fdb3877d2b4fe068",
     "grade": false,
     "grade_id": "best_model_gen_plot",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "##### Implement this part of the code #####\n",
    "raise NotImplementedError()\n",
    "# width = ?\n",
    "# model = ?\n",
    "\n",
    "X, y = convert_time_series_to_Xy(train_set, width)\n",
    "\n",
    "##### Implement this part of the code #####\n",
    "raise NotImplementedError()\n",
    "# model.fit( ? , ? )\n",
    "\n",
    "nof_predictions = 200\n",
    "prediction = predict(model, train_set, width, nof_predictions)\n",
    "\n",
    "plt.figure(figsize=(20,4))\n",
    "plt.plot(np.concatenate((train_set, prediction[:,0])), 'g')\n",
    "plt.plot(train_set, 'b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Altough the optimal result found here might not be the best visually, it is a far better result than the one you selected manually just because there was no cheating involved ;-).\n",
    "\n",
    "Some additional info:\n",
    "* This noise level of `RidgeCV()` and `LassoCV()` is estimated by automatically performing train and validation within the method itself. This will make them much more robust against over-fitting. The actual method used is [Cross-validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics)) which is a better approach of what we do here because it repeats the training and validation multiple times for different training and validation sets. The parameter that is set for these methods is often called the regularization parameter in literature and is well suited to avoid over-fitting.\n",
    "* Although sklearn supports estimating the noise level in Gaussian Processes it is not implemented within the method itself. Newer versions of sklearn seem to entail a lot of changes in this method so possibly it will be integrated in the (near) future. If you want to implement this noise level estimation yourself you can use [their cross-validation tool](http://scikit-learn.org/stable/modules/cross_validation.html) to set the [`alpha` parameter](http://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.GaussianProcessRegressor.html#sklearn.gaussian_process.GaussianProcessRegressor) in version 0.18 of sklearn. (The version used here is 0.17.)\n",
    "\n",
    "\n",
    "## 7. Classification\n",
    "\n",
    "In classification the purpose is to separate 2 classes. As an example we will use the double spiral. It is a very common toy example in machine learning and allows you to visually show what is going on.\n",
    "\n",
    "As shown in the graph below the purpose is the separate the blue from the red dots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Some code to generate spirals. You can ignore this for now.\n",
    "\n",
    "# To comply with standards in machine learning we use x1 and x2 as opposed to x and y for this graph \n",
    "# because y is reserved for the output in Machine Learning (= 0 or 1 in this case)\n",
    "\n",
    "r = np.arange(0.1, 1.5, 0.0001)\n",
    "theta = 2 * np.pi * r\n",
    "x1_0 = r * np.cos(theta)\n",
    "x2_0 = r * np.sin(theta)\n",
    "x1_1 = - r * np.cos(theta)\n",
    "x2_1 = - r * np.sin(theta)\n",
    "\n",
    "perm_indices = np.random.permutation(range(len(x1_0)))\n",
    "x1_0_rand = x1_0[perm_indices[ : 1000]] + np.random.randn(1000) / 5\n",
    "x2_0_rand = x2_0[perm_indices[ : 1000]] + np.random.randn(1000) / 5\n",
    "x1_1_rand = x1_1[perm_indices[1000 : 2000]] + np.random.randn(1000) / 5\n",
    "x2_1_rand = x2_1[perm_indices[1000 : 2000]] + np.random.randn(1000) / 5\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.scatter(x1_0_rand, x2_0_rand, color = 'b')\n",
    "plt.scatter(x1_1_rand, x2_1_rand, color = 'r')\n",
    "\n",
    "plt.plot(x1_0, x2_0, color = 'b', lw=3)\n",
    "plt.plot(x1_1, x2_1, color='r', lw=3)\n",
    "plt.xlim(-2, 2)\n",
    "plt.ylim(-2, 2)\n",
    "plt.xlabel('X1')\n",
    "plt.ylabel('X2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a colored image this is easy to do, but when you remove the color it becomes much harder. Can you do the classification in the image below?\n",
    "\n",
    "In black the samples from the train set are shown and in yellow the samples from the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a train and validation set\n",
    "\n",
    "x_train_0 = np.concatenate((x1_0_rand[ : 800].reshape((-1,1)), x2_0_rand[ : 800].reshape((-1,1))), axis=1)\n",
    "y_train_0 = np.zeros((len(x_train_0),))\n",
    "x_train_1 = np.concatenate((x1_1_rand[ : 800].reshape((-1,1)), x2_1_rand[ : 800].reshape((-1,1))), axis=1)\n",
    "y_train_1 = np.ones((len(x_train_1),))\n",
    "\n",
    "x_val_0 = np.concatenate((x1_0_rand[800 : ].reshape((-1,1)), x2_0_rand[800 : ].reshape((-1,1))), axis=1)\n",
    "y_val_0 = np.zeros((len(x_val_0),))\n",
    "x_val_1 = np.concatenate((x1_1_rand[800 : ].reshape((-1,1)), x2_1_rand[800 : ].reshape((-1,1))), axis=1)\n",
    "y_val_1 = np.ones((len(x_val_1),))\n",
    "\n",
    "x_train = np.concatenate((x_train_0, x_train_1), axis=0)\n",
    "y_train = np.concatenate((y_train_0, y_train_1), axis=0)\n",
    "\n",
    "x_val = np.concatenate((x_val_0, x_val_1), axis=0)\n",
    "y_val = np.concatenate((y_val_0, y_val_1), axis=0)\n",
    "\n",
    "# Plot the train and test data\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(x_train[:, 0], x_train[:, 1], color='k')\n",
    "plt.scatter(x_val[:, 0], x_val[:, 1], color='y')\n",
    "plt.xlim(-2, 2)\n",
    "plt.ylim(-2, 2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see classifying is very hard to do when you don't get the answer. But you will see that machine learning algorithms can solve this.\n",
    "\n",
    "### 7.1 Linear classifier\n",
    "\n",
    "Let's try to do this with a linear classifier.\n",
    "\n",
    "A linear classifier is basically a form of linear regression where the output is set to 1 for all the data points of class 1 and to 0 for all the data points of class 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "94014d918c602e443393ad858bc72ab4",
     "grade": false,
     "grade_id": "asdfasd",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "##### Implement this part of the code #####\n",
    "raise NotImplementedError()\n",
    "# model = sklearn.linear_model. ?\n",
    "# model.fit( ? )\n",
    "\n",
    "print 'The train accuracy is:', sklearn.metrics.accuracy_score(model.predict(x_train) > 0.5, y_train)\n",
    "val_score = sklearn.metrics.accuracy_score(model.predict(x_val) > 0.5, y_val)\n",
    "print 'The validation accuracy is:', val_score\n",
    "\n",
    "assert val_score > 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A quick and dirty helper function to plot the decision boundries\n",
    "def plot_decision_boundry(model, pol_exp=None):\n",
    "    x1 = np.random.permutation(np.linspace(-2, 2, num=20000)).reshape((-1, 1))\n",
    "    x2 = np.random.permutation(np.linspace(-2, 2, num=20000)).reshape((-1, 1))\n",
    "    x = np.concatenate((x1, x2), axis=1)\n",
    "    if pol_exp is None:\n",
    "        y = model.predict(x)\n",
    "    else:\n",
    "        y = model.predict(pol_exp.fit_transform(x))    \n",
    "    i_0 = np.where(y < 0.5)\n",
    "    i_1 = np.where(y > 0.5)\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.scatter(x[i_0, 0], x[i_0, 1], color='b', s=1)\n",
    "    plt.scatter(x[i_1, 0], x[i_1, 1], color='r', s=1)\n",
    "    plt.plot(x1_0, x2_0, color = 'b', lw=3)\n",
    "    plt.plot(x1_1, x2_1, color='r', lw=3)\n",
    "    plt.xlim(-2, 2)\n",
    "    plt.ylim(-2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_decision_boundry(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see a linear classifier returns a linear decission boundry.\n",
    "\n",
    "### 7.2 Non-linear classification\n",
    "\n",
    "Now let's do this better with a non-linear classifier using polynomials. Play with the degree of the polynomial expansion and look for the effect of the `RidgeCV()` and `LassoCV()` models. What gives you the best results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "98d5b718431f60aed2f1937edeb5ff86",
     "grade": false,
     "grade_id": "asdfas",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "##### Implement this part of the code #####\n",
    "raise NotImplementedError()\n",
    "# model = sklearn.linear_model. ?\n",
    "# pol_exp = sklearn.preprocessing.PolynomialFeatures(degree= ? )\n",
    "# model.fit( ? )\n",
    "\n",
    "print 'The train accuracy is:', sklearn.metrics.accuracy_score(model.predict(pol_exp.fit_transform(x_train)) > 0.5, y_train)\n",
    "val_score = sklearn.metrics.accuracy_score(model.predict(pol_exp.fit_transform(x_val)) > 0.5, y_val)\n",
    "print 'The validation accuracy is:', val_score\n",
    "\n",
    "plot_decision_boundry(model, pol_exp=pol_exp)\n",
    "\n",
    "assert val_score > 0.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything went well you should get a validation/test accuracy very close to 0.8.\n",
    "\n",
    "### 7.3 Random Forests\n",
    "\n",
    "An often used technique in machine learning are random forests. Basically they are [decision trees](https://en.wikipedia.org/wiki/Decision_tree_learning), or in programmers terms, if-then-else structures.\n",
    "\n",
    "Decision trees are know to over-fit a lot because they just learn the train set by hart. Random forests on the other hand combine multiple different (randomly initialized) decision trees that all over-fit in their own way. But by combining them they tend to cancel out eachothers mistakes. This approach is called an [ensemble](https://en.wikipedia.org/wiki/Ensemble_learning) and can be used for any combination of machine learning techniques.\n",
    "\n",
    "Now let's try to use a random forest to solve the double spiral problem. (see [`sklearn.ensemble.RandomForestClassifier()`](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "546cf15dff677fcc739f2dd5db20cfc5",
     "grade": false,
     "grade_id": "asdfa",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "import sklearn.ensemble\n",
    "\n",
    "##### Implement this part of the code #####\n",
    "raise NotImplementedError()\n",
    "# model = ?\n",
    "# model.fit( ? )\n",
    "\n",
    "print 'The train accuracy is:', sklearn.metrics.accuracy_score(model.predict(x_train) > 0.5, y_train)\n",
    "val_score = sklearn.metrics.accuracy_score(model.predict(x_val) > 0.5, y_val)\n",
    "print 'The validation accuracy is:', val_score\n",
    "\n",
    "plot_decision_boundry(model)\n",
    "\n",
    "assert val_score > 0.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see they are quite powerful right out of the box without any parameter tuning. But we can get the results even beter with some fine tuning.\n",
    "\n",
    "Try changing the `max_depth` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "7b30872882a3519dc2ba807f138504e8",
     "grade": false,
     "grade_id": "asdf",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "##### Implement this part of the code #####\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `max_depth` parameter limits the depth of the decision tree. The smaller this parameter is the less likely each tree will overfit.\n",
    "\n",
    "Now that you have found the optimal `max_depth` run the code again with the same parameter. Do you get the same result? Why not?\n",
    "\n",
    "Another parameter to play with is the `n_estimators` parameter. Play with only this parameter to see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "ffafec03d64a7332ecf51cd91d1c307e",
     "grade": false,
     "grade_id": "asd",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "##### Implement this part of the code #####\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see increasing the number of estimators improves the model and reduces over-fitting. This parameter actually sets the number of trees in the random forest. The more trees there are in the forest the better the result is. But obviously it requires more computing power so that is the limiting factor here.\n",
    "\n",
    "This is typical for ensembles. The idea is that if you combine more fools you get a good result on average.\n",
    "\n",
    "Now try combining the `n_estimators` and `max_depth` parameter below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "b59b110f1ac2cdf4ee5ff20a241e9975",
     "grade": false,
     "grade_id": "as",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "##### Implement this part of the code #####\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "As you have notised by now it seems that random forests are less powerful than non-linear regression with polynomial feature extraction. This is because these polynomials are ideally suited for this task. This also means that you could get an even better result if you would also apply polynomial expansion for random forest. Try that below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "88113b097e9357adafceb60416ce880d",
     "grade": false,
     "grade_id": "a",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "##### Implement this part of the code #####\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "As you have notised it is still not possible to get the same result as linear regression. This is just to show that linear techniques are very powerful and often underrated. But in some situations they are not powerful enough and you need something stronger like a random forest or even neural networks.\n",
    "\n",
    "There is one neat trick in the random forests. If you set the `n_jobs` it will use more than 1 core to compute. Set it to -1 to use all the cores (including hypertrheading cores). But don't do that during this tutorial because that would block the machine you are all working on.\n",
    "\n",
    "To avoid over-fitting you have set the `max_depth` parameter for random forests which sets the maximum depth of the tree. Instead you can also set the `min_samples_split` parameter which determines while building the tree how many data points you need at least before you create another split (this is an additional if-else structure). Or the `min_samples_leaf` that sets the minimum amount of data points you have in each leaf. All 3 parameters are dependent on the number of data points in your dataset especially the last 2 so don't forget to adapt them if you have been playing around with a small subset of the data. In my experience the 2 last parameters give slightly better results and it ususally doesn't make sense to combine them.\n",
    "\n",
    "In the previous exercises we have done a lot of the optimazations on the test set. This should of course be avoided. What you should do instead is to optimize and select your model using a validation set and of course you should automate this proces as shown in one of the earlier exercises. One thing to take into account here is that you should use multiple initialisation of a random forest because the decision trees is randomly generated.\n",
    "\n",
    "# 8. Main take home messages\n",
    "\n",
    "Because we can't cover everything, we listed all the basics of what you should take home before working on your own machine learning project below.\n",
    "\n",
    "### 8.1 The basic rules of machine learning\n",
    "\n",
    "Any good club has its own set of rules. The rules for machine learning club are the following:\n",
    "\n",
    "* First rule of ML is: Over-fitting is a real problem and try anything to avoid it\n",
    "* Second rule of ML is: You are probalby over-fitting. Are you sure you are not fitting on your test data?\n",
    "* Third rule of ML is: You think over-fitting will not happen to you, but it is happening right now!\n",
    "* Fourth rule of ML is: Talk about it with your peers because over-fitting is a real issue.\n",
    "\n",
    "### 8.2 My winning strategy\n",
    "\n",
    "Although I'd like to claim it as mine, it is a general (non-written) consensus amongst data scientists to use the following approach. Even experts should not skip any of the steps below.\n",
    "\n",
    "1. Create a train set and a test set\n",
    "* Rescale your train set to zero-mean-unit-variance (most methods assume gaussian distributed data)\n",
    "* Don't look at the test set\n",
    "* Implement a cross-validation framework\n",
    "* Try **linear regression with regularisation** for regression and classification (`RidgeCV` or `LassoCV`)\n",
    "* Try techniques to avoid over-fitting\n",
    "* Check the validation score\n",
    "* If the results are not optimal and there is no overfitting going on try **adding features** else go to step 17\n",
    "* Rescale your features to zero-mean-unit-variance (most methods assume gaussian distributed data) or select those features that have this property\n",
    "* Try techniques to avoid over-fitting (including removing features for more info see [feature selection techniques](http://scikit-learn.org/stable/modules/feature_selection.html))\n",
    "* Check the validation score\n",
    "* If the results are not optimal and there is no overfitting going on try **random forrests** else go to step 17\n",
    "* Try techniques to avoid over-fitting\n",
    "* Check the validation score\n",
    "* If the results are not optimal and there is no overfitting going on try **neural networks** or deep learning else go to step 17\n",
    "* Try techniques to avoid over-fitting\n",
    "* Only in the end check the score on the test set and make sure it is similar to the validation score. Otherwise you have been overfitting and you need to take a couple steps back.\n",
    "* Make an ensemble of your best (but significantly different) methods\n",
    "* Finally build the model using all the data available and run it in production\n",
    "\n",
    "You can try other machine learning techniques, but usually the difference is quite small. So don't waste too much time on getting to know them because they all have their own quirks and specific ways of over-fitting. Besides maybe most important of all, Kaggle competitions are usually won with one of these techniques.\n",
    "\n",
    "### 8.3 How to avoid over-fitting\n",
    "\n",
    "As you should know by now over-fitting is one of the biggest issues in machine learning. So pay attention for it. \n",
    "\n",
    "Below you can find some of the most common techniques to avoid over-fitting:\n",
    "\n",
    "* Use more data\n",
    "* Artificially generate more data based on the orriginal data\n",
    "* Use a smaller model (with less parameters)\n",
    "* Use less features (and thus less parameters)\n",
    "* Use a regularisation parameter\n",
    "* Artificially add noise to your model\n",
    "* Only use linear models (or in neural networks make sure that the non-linearity in your model is closer to a linear function)\n",
    "* Combine multiple models that each over-fit in their own peculiar way into what is called an ensemble\n",
    "\n",
    "\n",
    "### 8.4 Most common features\n",
    "\n",
    "Although there is no general rule to which features you should use, there are a couple of features that come back regularly:\n",
    "\n",
    "* Log: Take the log of the data to make it more Gaussian. This works best for data that is exponentially or log-normally distributed\n",
    "* Polynomials: The square is quite common but higher orders are often used as well\n",
    "* Differentials: The first and sometimes the second derivative are used (see [`numpy.diff()`](https://docs.scipy.org/doc/numpy-1.10.1/reference/generated/numpy.diff.html))\n",
    "* Integrals (use [`numpy.sum()`](https://docs.scipy.org/doc/numpy-1.10.1/reference/generated/numpy.sum.html) for example to implement it)\n",
    "* Mean: Often used to smooth the data\n",
    "* Median: Same as the mean but this ignores outliers\n",
    "* Standard deviation or variance\n",
    "* Skewness and kurtosis: These are rarely used but sometimes they contain valuable information\n",
    "* Fourrier transform: If your data contains a frequency spectrum. Typically used when processing speech and sound. (see [`numpy.fft.fft()`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.fft.fft.html#numpy.fft.fft))\n",
    "* Frequency filtering: Similar to the fourrier transform (see [`scipy.signal.butter()`](https://docs.scipy.org/doc/scipy-0.18.1/reference/generated/scipy.signal.butter.html#scipy.signal.butter))\n",
    "* Spatial filters: Are often used for images. Edge detectors for example\n",
    "* Any other feature that seems to make sense regarding your data\n",
    "\n",
    "\n",
    "## Feedback\n",
    "\n",
    "If you have any feedback regarding this tutorial, feel free to share it with us. You can mail to `pieter.buteneers@gmail.com`."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
