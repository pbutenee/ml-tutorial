{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis and Feature Engineering Workshop\n",
    "\n",
    "#### Created by:\n",
    "\n",
    "- Vladimir Rybakov, Head of Data Science at [Wave Access](https://www.wave-access.com/)\n",
    "- Pieter Buteneers ([@PieterButeneers](https://twitter.com/pieterbuteneers)), Director of Engineering in ML & AI at [sinch.com](https://www.sinch.com/) \n",
    "\n",
    "In this workshop you will learn about different methods of data preprocessing and analysis.\n",
    "\n",
    "Agenda:\n",
    "\n",
    "1. Cleaning and transforming data, preparing it for further processing: removing missing values, correcting errors, removing outliers and errors, converting data types, etc. \n",
    "\n",
    "2. Exploratory data analysis: correlation analysis, variable relationship analysis. \n",
    "\n",
    "3. Preparing the feature space: encoding categorical variables, manual feature generation, dimensionality reduction and feature selection.\n",
    "\n",
    "4. Training the model.\n",
    "\n",
    "For this workshop we will be using a dataset, which contains records of approximately 10 000 applications scraped from Google Play Store. This  will allow us to demonstrate different data processing and feature engineering techniques in conditions similar to a real project (such, that data scientists have to deal with in their work all the time).\n",
    "We will try to answer the question: what factors determine the rating of a particular application and whether this rating can be predicted in advance?\n",
    "\n",
    "<a id='attention'></a>\n",
    "\n",
    "**!! Attention !!**\n",
    "\n",
    "In order to slightly simplify the process, one assumption was made. We will perform all actions on the whole dataset and will split it into training and test subsets only before actually fitting the model. This approach allows us to avoid writing a large number of functions to convert the test dataset to the format of the training and simplifies the understanding of the performed actions. \n",
    "\n",
    "However, it is only acceptable on educational tasks. When working with real data, test and validation datasets should be allocated in advance. All data analysis is performed only on the training subset (except checking data distributions). The validation and test subsets are transformed to the training format (to the same format: only those categories that are in the training set are left, missing values are processed in the same way, etc.)\n",
    "\n",
    "### Libraries\n",
    "\n",
    "We will use the libraries dividing into the following categories:\n",
    "\n",
    "<a id='lib_data_manipulation'></a>\n",
    "#### data preprocessing and descriptive statistics\n",
    "\n",
    "> **pandas** - a library for data manipulation: dataset transformation, descriptive statistics, working with time series, simple visualization.\n",
    "\n",
    "> **numpy** - a library for working with numeric arrays, convenient in combination with pandas, as it uses less resources.\n",
    "\n",
    "> **scipy** - a library that allows applying mathematical functions, linear algebra, statistical tests to data.\n",
    "\n",
    ">**statsmodels** - an add-on over SciPy that simplifies the application of some statistical models and makes visualization more illustrative.\n",
    "\n",
    "<a id='lib_data_visualization'></a>\n",
    "#### data visualization\n",
    "\n",
    "> **matplotlib** - the main library for data visualization, has a rich functionality, although interface is a little bit outdated.\n",
    "\n",
    "> **seaborn** - an add-on over matplotlib, makes graphics more informative and visually attractive.\n",
    "\n",
    "<a id='lib_machine_learning'></a>\n",
    "#### machine learning\n",
    "\n",
    "> **scikit-learn (sklearn)** - a library containing a large number of algorithms for solving machine learning problems: data preprocessing, implementation of different models, toy datasets.\n",
    "\n",
    ">**umap** - implementation of the \"Uniform manifest Approximation and Projection\" algorithm. Allows you to visualize multidimensional data by lowering its dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import seed\n",
    "\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import uniform, truncnorm, randint\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix, accuracy_score, roc_auc_score\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error, make_scorer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import sklearn.cluster as cluster\n",
    "import umap\n",
    "\n",
    "from joblib import load\n",
    "from io import BytesIO\n",
    "import requests\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import rcParams\n",
    "import seaborn as sns\n",
    "plt.style.use('seaborn-poster')\n",
    "%matplotlib inline\n",
    "\n",
    "import os \n",
    "import pickle\n",
    "import warnings\n",
    "from time import time\n",
    "warnings.filterwarnings('ignore') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1_data_cleaning'></a>\n",
    "## 1. Exploring and  transforming the variables\n",
    "\n",
    "<a id='surface_examination'></a>\n",
    "### 1.1. Brief data overview\n",
    "\n",
    "First, we should load the data and visually explore at least a few lines, see the types of variables, the number of samples in order to get the general understanding of data and get the feeling of what you are working with.\n",
    "\n",
    "Let's define the paths to the data. It is better to do this at the beginning so, if in the future we need to change them, you won't have to search through the entire script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_F ='./data/googleplaystore_alter.csv'\n",
    "FEATURE_IMPORTANCE_F= './data/feature_importances_logreg.csv'\n",
    "BONUS_F = './data/bonus_df_alter.csv'\n",
    "CLUSTER_F = './data/cluster.joblib?raw=true'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construction \"./\" means we use the current folder of the script. \"../\" would mean - one level higher relative to the script. In our case we want to stay in the same folder where the script file is located, and go to the \"data\" folder. \n",
    "\n",
    "Pandas uses two data structures to store data:\n",
    "\n",
    "**Series** - is an object similar to ordinary one-dimensional array, with the difference that each element has its own index. Moreover, Series has a stored data type.\n",
    "\n",
    "**DataFrame** - is a tabular data structure where each column is a Series.\n",
    "\n",
    "Let's load *googleplaystore_alter.csv* into variable *df* with *pandas* function *read_csv()*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATA_F)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1:** Bring out the first (or last) **ten** rows from dataset using methods [head()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.head.html) or [tail()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.tail.html#pandas.DataFrame.tail). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d3075ab69644d15d318e6eda09891c38",
     "grade": false,
     "grade_id": "cell-728093097fee2789",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "##### Implement this part of the code #####\n",
    "raise NotImplementedError(\"Code not implemented, follow the instructions.\")\n",
    "# df.head(?)\n",
    "# df.tail(?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Dataset Descriptions:**\n",
    "\n",
    "+ App - application name;\n",
    "\n",
    "+ Category - application category;\n",
    "\n",
    "+ Rating - application rating;\n",
    "\n",
    "+ Size - application size;\n",
    "\n",
    "+ Type - application type (paid or free);\n",
    "\n",
    "+ Price - application price;\n",
    "\n",
    "+ Content Rating - the age group for which the application is intended;\n",
    "\n",
    "+ Genres - subcategories to which the application belongs;\n",
    "\n",
    "+ Current Ver - current version of application;\n",
    "\n",
    "+ Android Ver - the minimum OS version required for the application works.\n",
    "\n",
    "*DataFrame* objects have dual indexing - by columns and by rows. \n",
    "To select a specific row you can use *loc* (returns rows by a given index) or *iloc* (returns rows by their position in the dataset) methods:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should be noticed that loc and iloc methods are most likely to return different values. For example, let's select the 3rd line.\n",
    "\n",
    "We can also select a specific column. With the head function you can print the colum names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['App'].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *at* method is used for quick selection of the specific element:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.at[1, 'App']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you need to select several columns, you could do it by feeding a list. \n",
    "\n",
    "For several rows you can use a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[1:3, ['App', 'Category']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or you can use a slice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[2:4, [1, 2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you need strings that match a condition (for example, only applications of the ART_AND_DESIGN category), the following query format is used (it also returns a DataFrame object):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['Category']=='ART_AND_DESIGN'].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we used the head() function again to only show the first 5 rows).\n",
    "\n",
    "For complex conditions, each expression is placed in parentheses and the & or | operators are used. For example, we can find the applications from the category ART_AND_DESIGN with the type Free:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df['Category']=='ART_AND_DESIGN') & (df.Type == 'Free')].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the general information about dataset using the *info()* method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset consists of 13 columns, only one of which is numeric (float64) and the rest are categorical (object). The dataset has 10841 rows. Note that there are also missing values, mostly in the Current Ver.\n",
    "\n",
    "*Pandas* contains many built-in functions for working with data. You can learn more about them here: [pandas](https://pandas.pydata.org/pandas-docs/stable/)\n",
    "\n",
    "<a id='data_studying'></a>\n",
    "### 1.2. Exploring the data \n",
    "\n",
    "The data may contain incorrect or missing values and other artifacts. Moreover, types may not match the content. In order to proceed, we must find and correct all errors. In addition, it is necessary to individually explore every features distribution. \n",
    "\n",
    "Based on the research results and some logical considerations, we should decide how to use those features: remove, leave, replace continuous values with classes or categories, reduce the existing number of categories, apply a mathematical function to the values, and so on.\n",
    "\n",
    "<a id='category'></a>\n",
    "#### Category\n",
    "\n",
    "Let's look at the distribution of applications by category using the *value_counts()* method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Category.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most applications are in FAMILY, GAME and TOOLS  categories.\n",
    "\n",
    "There is only one application in  \"2\" category. In addition, this name does not make sense in this context. Let's take a look at this app:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.Category == '2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There was a shift of a row on one column to the left. Let's delete this sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.Category != '2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2:** Are there apps with missing category values? You can use methods: [isnull](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.isnull.html) and  [sum](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.sum.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2210a7cb13ea077262856585f2d03fea",
     "grade": false,
     "grade_id": "cell-e21c22e383449a79",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "##### Implement this part of the code #####\n",
    "raise NotImplementedError(\"Code not implemented, follow the instructions.\")\n",
    "# df.Category. ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='rating'></a>\n",
    "#### Rating\n",
    "\n",
    "Rating - is our target column. It's important to check that all the values are alright.\n",
    "\n",
    "**Task 3** What rating does the majority of apps have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c3a90b9aefb0098e37f5619ceaf97cae",
     "grade": false,
     "grade_id": "cell-f44f5023a46e66fb",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "##### Implement this part of the code #####\n",
    "raise NotImplementedError(\"Code not implemented, follow the instructions.\")\n",
    "# df.Rating. ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,7))\n",
    "plt.pie(df.Rating.value_counts(),\n",
    "        labels=df.Rating.value_counts().index,\n",
    "        autopct='%1.1f%%',\n",
    "        startangle=120, \n",
    "        explode=[0.02]*3)\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# df.Rating.astype(int)\n",
    "# df.Rating.plot.pie()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Rating.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking for missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Rating.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='size'></a>\n",
    "#### Size\n",
    "\n",
    "The information contained in Size is clearly numeric, however the column is categorical because it contains special characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Size.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The size of applications is specified in megabytes and kilobytes. But the most common is the value 'Varies with device'.\n",
    "\n",
    "Because of the characters M (megabytes) and K (kilobytes), *pandas* will not be able to automatically translate them into numbers and will throw an error. So, it's not possible just change the column type \"Size\" from object to float using the astype('float') method. \n",
    "\n",
    "Let's transfrom all values to the amount of megabytes and then to the number format.\n",
    "\n",
    "**Task 4** Write a function that will receive one of the possible values of the Size column to the input, and will return either the corresponding number of megabytes or np.NaN. Use function [string.replace()](https://www.geeksforgeeks.org/python-string-replace/). \n",
    "\n",
    "*1Mb = 1024Kb*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cd8e1db532801c93a229b7b90cd71c37",
     "grade": false,
     "grade_id": "cell-6f5cae012ba5cb90",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "##### Implement this part of the code #####\n",
    "raise NotImplementedError(\"Code not implemented, follow the instructions.\")\n",
    "# def check_size(size_value):\n",
    "#  write your code here  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's apply our function to the Size column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Size = df.Size.apply(check_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that the column is in the float format now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert df.Size.dtype == float\n",
    "df.Size.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All values 'Variations with device' are missing now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Size.isnull().sum()/len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see there is about 16% of the samples with NaN values (previously 'Variations with device'). That's quite a lot. To deal with this, we can create a new column with binary values that will store information about these NaN samples (perhaps this information will be useful) and then process them in the original column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['unknown_size'] = df.Size.isnull()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's handle the missing values. There are several traditional methods to deal with them:\n",
    "\n",
    "1. remove\n",
    "2. replace with average, mode, median, 0\n",
    "3. replace with nearby examples (mostly for time series data)\n",
    "4. use the distribution of existing data and use it to generate new values for features. (useful when using linear models)\n",
    "5. make this column a target variable and train a separate model to predict the missing values\n",
    "\n",
    "The easiest option is to delete these rows. But 16% is a lot of samples. We don't want to lose so much useful information. The best way is to use a special model, but it can take a lot of time. Usually it's better to compare different approaches.\n",
    "\n",
    "For this case we will use the median, but you can try experimenting with other options.\n",
    "\n",
    "**Task 5** Fill in the missing values in the Size column with the median value of the same column. Use the methods [fillna](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.fillna.html) и [median](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.median.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f73a73d3fc3d7aa3bbca718aba4386e3",
     "grade": false,
     "grade_id": "cell-7d68ec917c43cc1d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "##### Implement this part of the code #####\n",
    "raise NotImplementedError(\"Code not implemented, follow the instructions.\")\n",
    "# df.Size = df.Size.fillna(value=?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just in case, check that there are no empty values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert df.Size.isnull().sum() == 0\n",
    "df.Size.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='type'></a>\n",
    "#### Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Type.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "93% of apps are free. To say more, you need to look at the relationships with other variables. We will deal with this in the next section. Let's check for missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Type.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is one missing, let's inspect this one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.Type.isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 6** Handle the instance with the missing \"Type\" in the best way on your opinion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f80fd9855dcc4c60383d20db69e597af",
     "grade": false,
     "grade_id": "cell-9dca9f1a12cb3fbe",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "##### Implement this part of the code #####\n",
    "raise NotImplementedError(\"Code not implemented, follow the instructions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert df.Type.isnull().sum() == 0\n",
    "df.Type.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='price'></a>\n",
    "#### Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Price.value_counts()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable is numeric, but it contains special characters. That's why it has 'object' Type.\n",
    "\n",
    "**Task 7:** Remove the special character \"$\". Change the column type to 'float' using the method [astype()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.astype.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cc7fcccf589d33427fe87a4940572f40",
     "grade": false,
     "grade_id": "cell-6d603c590f3a9b2c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "##### Implement this part of the code #####\n",
    "raise NotImplementedError(\"Code not implemented, follow the instructions.\")\n",
    "# df['Price'] = df['Price'].str.replace(?)\n",
    "# df['Price'] = df['Price']. ?\n",
    "\n",
    "assert df.Price.dtype == float"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a distribution plot of prices among the paid apps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[15, 7])\n",
    "sns.distplot(df.Price[df.Type == 'Paid'])\n",
    "plt.xlabel(\"Price, $\")\n",
    "plt.title('The distribution of apps by price' ,size = 24)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cheap apps make up the most part. Moreover, the rest of apps take too small part of all apps, so it is difficult to say anything by the graphic. \n",
    "Let's look at logarithm of price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[15, 7])\n",
    "sns.distplot(np.log(df.Price[df.Type == 'Paid']))\n",
    "plt.xlabel(\"log(Price), $\")\n",
    "plt.title('The distribution of apps by price',size = 24)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most apps cost around 1 and 3 dollars. The dispersion of values on the left part of the plot is due to the strong discreteness of small values of the original column. Let's have a look at the strange peak on the right side of the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.Price > 200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strange apps for the rich. Perhaps they are an indicator of status in certain circles or something like that.\n",
    "Let's check for missing values and move on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Price.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='content_rating'></a>\n",
    "#### Content Rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Content Rating'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the \"Adults only 18+\" and \"Unrated classes\". They are too few."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[(df['Content Rating'] != 'Adults only 18+') & (df['Content Rating']!= 'Unrated')] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Content Rating'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Content Rating'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='genres'></a>\n",
    "#### Genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Genres.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "THere are quite a lot of genres - 119. Moreover, some of them are actually a result of combining two basic genres. Let's find out the amount of unique ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_genres = []\n",
    "for i in df.Genres.str.split(';').values:\n",
    "    list_of_genres.extend(i)\n",
    "print('Amount of subcategory: {}'.format(len(set(list_of_genres))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 8** Calculate the amount of genres for each app and estimate the general distribution for them? Use [series.str.findall()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.str.findall.html)\n",
    "\n",
    "Tip: use the apply function to find the length to keep this a one-liner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3bd788a57b13e087a5c0b3e16863419a",
     "grade": false,
     "grade_id": "cell-a82f26496d503055",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "##### Implement this part of the code #####\n",
    "raise NotImplementedError(\"Code not implemented, follow the instructions.\")\n",
    "# counts_of_genres = df.Genres.str.findall ?\n",
    "\n",
    "assert sum(counts_of_genres) == 11333"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_of_genres.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(counts_of_genres>1).sum()/len(counts_of_genres)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There isn't a lot of applications with many genres. Let's look at some of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[counts_of_genres>1].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may notice that the genre column often contains the same as the category column. We will explore this in more detail later. Now let's check for missing values and move on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Genres.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='current_ver'></a>\n",
    "#### Current Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Current Ver'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of too many unique values, this isn't a particularly informative column. It seems that each developer uses its own notation, so we will just remove it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['Current Ver'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='android_ver'></a>\n",
    "#### Android Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Android Ver'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Android Ver'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 2 missing values. Remove it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['Android Ver'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's cluster them together in bigger classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 9):\n",
    "    df.loc[df['Android Ver'].str.contains('^{}..*'.format(i)), 'Android Ver'] = '{} and up'.format(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Android Ver'].value_counts(normalize=True).sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Groups 1-3 and 5-8 still have a small number of examples. The biggest one is 4 group. We probably should cluster the classes even more and combine 1-3 and 5-8 into two separate groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['Android Ver'].str.contains('^[123]..*'), 'Android Ver'] = '1 and up'\n",
    "df.loc[df['Android Ver'].str.contains('^[5678]..*'), 'Android Ver'] = '5 and up'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Android Ver'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With such distribution of values, this feature can actually contribute to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Android Ver'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='final_check'></a>\n",
    "#### Final check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сheck if there are missing values in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='removing_duplicates'></a>\n",
    "### 1.3. Removing duplicates\n",
    "\n",
    "Now we should make sure there are no applications that occur in the dataset more than once, or in other words - duplicate each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dup = df[df.duplicated(subset='App')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dup.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dup_apps = ['Box', 'Call Blocker', 'Bubble Shooter', 'Word Search']\n",
    "\n",
    "df_tmp = pd.DataFrame()\n",
    "for col in dup_apps:\n",
    "    df_tmp = pd.concat((df_tmp, df[df.App == col]))\n",
    "df_tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some applications have duplicates with different sizes, so we’ll sort by size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values('Size')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 9** Remove duplicates with [drop_duplicates()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.drop_duplicates.html) (keep applications with the largest size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f198074d250004afbf09f5dc1c875631",
     "grade": false,
     "grade_id": "cell-b921bda0e50dad22",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "##### Implement this part of the code #####\n",
    "raise NotImplementedError(\"Code not implemented, follow the instructions.\")\n",
    "# df = df.drop_duplicates(?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert df.shape == (9652, 10)\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After filtering, we lost ~11% from the total volume of our dataset. Obviously this is an unpleasant measure, but it has to be done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2_data_relations'></a>\n",
    "## 2. Exploring the relationship between variables\n",
    "\n",
    "<a id='numerical_features'></a>\n",
    "###  2.1. Numerical features\n",
    "\n",
    "One of the simple and effective methods to explore the relationship between numerical features is correlation analysis. \n",
    "\n",
    "> **Correlation**  - is a statistical relationship between two or more random variables (or variables that can be considered as random with some acceptable degree of accuracy). Changes in one or more values lead to a systematic changing of another values.\n",
    "\n",
    "A correlation matrix is a matrix that contains the values of correlations between variables. In our case the correlation values between all pairs of numerical features will be obtained.\n",
    "\n",
    "<img src=\"./images/Correlation_examples.png\">\n",
    "\n",
    "The image above give you some insight into what the correlation values mean. As you can see this only works in sort of a straight line. Non-linear correlations, as shown in the bottom row, are not captured by this method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_df = df.corr()\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "mask = np.zeros_like(corr_df, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "plt.subplots(figsize=[15,10])\n",
    "plt.title('Correlation matrix')\n",
    "sns.heatmap(corr_df, mask=mask, cmap=cmap, linewidths=.5, annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ There is no correlation between the numeric variables.\n",
    "\n",
    "As the next step we will use *pairplot* method for our numeric variables. In order to be able to tell something we shall consider *price* on a logarithmic scale. Let's make a separate dataframe for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_log = df.copy()\n",
    "df_log['log_price'] = np.log1p(df_log.Price)\n",
    "df_log.unknown_size = df_log.unknown_size.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Pairplots* compares the distributions of variables in pairs and allow you to make the most common assumptions, which can then be checked and clarified later.\n",
    " \n",
    "The distributions of the variables are shown on the diagonal with a color breakdown by the selected types. Relations between two variables are drawn above and below the diagonal: each point is one application in our case.\n",
    "\n",
    "By default, pairplots use KDE. But it is often built incorrectly - instead of smooth \"bells\" there are thin high peaks, ridges and other artefacts. Therefore, the using of histograms is more reliable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.pairplot(df_log, hue='Rating', \n",
    "                  vars = ['Size', 'log_price', 'unknown_size'],\n",
    "                  plot_kws = {'alpha': 0.6, 's': 80, 'edgecolor': 'w'},\n",
    "                  diag_kind='hist', diag_kws = {'edgecolor': 'w', 'alpha': 0.6, 'bins': 10}, \n",
    "                  size = 4)\n",
    "ax.fig.suptitle('Pairplot with grouping by rating', y=1.02, size=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 10:** Explore the graphics. What conclusions can you make?\n",
    "\n",
    "**Answer** We can see from graphics the following:\n",
    "+ Applications with a small size and \"0\" rating are more frequent than others. Applications with other sizes have more instances with rating \"2\" \n",
    "+ No apps with big size and high price\n",
    "+ Apps with \"2\" rating mostly have low prices. The peak on the 2nd basket probably occurred due to the replacement of missing \"size\" samples with the median.\n",
    "+ Applications with \"2\" rating stand out among the applications whose size depends on the device \n",
    "\n",
    "<a id='app_len_&_rating'></a>\n",
    "### 2.2. Application length and rating\n",
    "\n",
    "Let's see how the length of the application name affects the rating. To do this, create a separate dataframe with the App and Rating columns. As a result we will receive a column with the length of the application name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_app = pd.DataFrame({'App': df.App,\n",
    "                       'Rating': df.Rating})\n",
    "df_app['App_len'] = df_app.App.apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 7))\n",
    "for i in sorted(df_app.Rating.unique()):\n",
    "    sns.kdeplot(df_app.App_len[df_app.Rating==i],\n",
    "                shade=True,\n",
    "                legend=False,)\n",
    "plt.legend(labels=sorted(df_app.Rating.unique()))\n",
    "plt.xlabel(\"Number of characters in App\")\n",
    "plt.title(\"The distribution of the Apps name\",size = 18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apps with \"0\" rating are more likely to have a short title. At the same time, more popular applications are in the range of 20 to 50 characters.\n",
    "\n",
    "<a id='type_&_price'></a>\n",
    "### 2.3. Price and Type\n",
    "\n",
    "It is logical to assume that if \"Type\" is \"Free\", then \"Price\" should be equal to zero.\n",
    "\n",
    "Let's group \"Type\" and look at the average price in each group (\"Free\" and \"Paid\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('Type')['Price'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average price of a \"Free\" app is zero dollars, a paid one is 14 dollars. Looks fine.\n",
    "\n",
    "<a id='category_&_genres'></a>\n",
    "###  2.4. Category and Genres\n",
    "\n",
    "Genres is a subsection of Category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see which categories use the most subcategories.\n",
    "Next we will aggregate categories and genres in a separate table and make a new column with the number of genres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat_genrs = df[['Category', 'Genres', 'Rating']]\n",
    "df_cat_genrs['Count_of_genres'] = df_cat_genrs.Genres.str.findall(';').apply(len)+1\n",
    "df_cat_genrs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 11** Explore the genre number statistics for each category. Use [groupby](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.groupby.html) и [describe](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.describe.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8cc03582fc639d156bc1ba6250f39346",
     "grade": false,
     "grade_id": "cell-9df1d61cd84c8c6c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "##### Implement this part of the code #####\n",
    "raise NotImplementedError(\"Code not implemented, follow the instructions.\")\n",
    "# group_df = df_cat_genrs.groupby( ? )[ ? ].describe().sort_values('mean', ascending=False)\n",
    "group_df.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The maximum number of subcategories in one category is 2, the minimum is 1. Two subcategories are most often found in the categories PARENTING, FAMILY, EDUCATION. Only 11 categories have multiple genres.\n",
    "\n",
    "Categories and genres often overlap. Let's study this more in detail.\n",
    "\n",
    "For starters we should bring categories and genres to a common format: transform them into lower case (*str.lower()*) and remove special characters (replace the special characters with spaces or nothing using the *str.replace()*).\n",
    "\n",
    "\n",
    "_**Note:** pandas has many methods for working with strings. You can read more about them in the documentation_\n",
    "[Working with Text Data](https://pandas.pydata.org/pandas-docs/stable/user_guide/text.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat_genrs.Category = df_cat_genrs.Category.str.lower()\n",
    "df_cat_genrs.Genres = df_cat_genrs.Genres.str.lower()\n",
    "df_cat_genrs.Genres = df_cat_genrs.Genres.str.replace('&', '')\n",
    "df_cat_genrs.Genres = df_cat_genrs.Genres.str.replace(';', ' ')\n",
    "df_cat_genrs.Category = df_cat_genrs.Category.str.replace('_', ' ')\n",
    "df_cat_genrs.Category = df_cat_genrs.Category.str.replace('and', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat_genrs.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate how many apps have the same genres and categories. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_cat_genrs['Category'] == df_cat_genrs['Genres']).sum()/len(df_cat_genrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns are completely the same for almost 70%. That is, 70% of the values don't contain new information. Most likely, there are more matches considering possible errors related to usage of regular expressions on raw data.\n",
    "\n",
    "Let's add a column to indicate whether the genre is equal to the category or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat_genrs['is_cat_equal_genre'] = df_cat_genrs['Category'] == df_cat_genrs['Genres']\n",
    "df_cat_genrs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 12** Explore whether there is a difference between distributions of apps with the same and with different categories and genres (use [sns.countplot](https://seaborn.pydata.org/generated/seaborn.countplot.html))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "69a583519b9580e5464dc876db0298b7",
     "grade": false,
     "grade_id": "cell-d4329df81304b81c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "##### Implement this part of the code #####\n",
    "raise NotImplementedError(\"Code not implemented, follow the instructions.\")\n",
    "# sns.countplot(x= ?,  hue='Rating', data=df_cat_genrs, hue_order=np.sort(df['Rating'].unique()))\n",
    "\n",
    "plt.xlabel(\"Coincidence of Genre and Category\")\n",
    "plt.title('The distribution of apps by ratings',size = 18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rating distributions for True and False are slightly different. You can see that applications with a rating 0 stand out a little bit. To numerically estimate this distribution, we will use the contingency table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cr_tab = pd.crosstab(df_cat_genrs.Rating, df_cat_genrs.is_cat_equal_genre)\n",
    "cr_tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cr_tab / cr_tab.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference in distribution for apps with same category and genre is negligible. On the other side, applications with different genre and category are 1.5 times more likely to have \"2\" rating.\n",
    "\n",
    "Such column could be useful in predicting the rating of applications. At the same time there is little sense to leave two columns with almost the same information. That's why we remove the column genre and add column is_cat_equal_genre in the main dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['Genres'])\n",
    "df['is_cat_equal_genre'] = df_cat_genrs['is_cat_equal_genre'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='rating_&_content_rating'></a>\n",
    "### 2.5. Rating and Content Rating\n",
    "\n",
    "**Task 13** Explore the relationship between rating and content rating columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for task version - hide next 3 cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "481d1cbbfdf0a06d6e10b2919ea12faa",
     "grade": false,
     "grade_id": "cell-59f8e659e08cba56",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "##### Implement this part of the code #####\n",
    "raise NotImplementedError(\"Code not implemented, follow the instructions.\")\n",
    "# sns.countplot(x= ?, hue= ?, data=df, hue_order=np.sort(df['Rating'].unique()))\n",
    "\n",
    "plt.xlabel(\"Сontent Rating\")\n",
    "plt.title('The distribution of apps by ratings',size = 18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0b8f6567d80bbc1c92cd8276e1f70966",
     "grade": false,
     "grade_id": "cell-fd40d4e5088fffee",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "##### Implement this part of the code #####\n",
    "raise NotImplementedError(\"Code not implemented, follow the instructions.\")\n",
    "# df_ct_rc = pd.crosstab(?, ?)\n",
    "\n",
    "df_ct_rc /= df_ct_rc.sum()\n",
    "df_ct_rc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "for i, con_rating in enumerate(df['Content Rating'].unique()):\n",
    "    plt.subplot(2, 2, i+1)\n",
    "    vc = df.Rating[df['Content Rating']== con_rating].value_counts().sort_index()\n",
    "    patches = plt.pie(vc,  autopct='%1.1f%%', \n",
    "           startangle=120, explode=[0.02]*3)\n",
    "    plt.title(con_rating)\n",
    "    plt.axis('equal')\n",
    "plt.legend(labels=vc.index, loc=(1.02,1.75))\n",
    "\n",
    "   \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the table of both at the rating contingency with Content Rating column and the is_not_equal_genre column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_ct_comp = pd.crosstab(df.Rating, [df['Content Rating'], df.is_cat_equal_genre])\n",
    "pd_ct_comp /= pd_ct_comp.sum()\n",
    "pd_ct_comp = pd_ct_comp.style.background_gradient(cmap='summer_r')\n",
    "pd_ct_comp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First thing you could notice are applications with the age rating \"Everyone\" and the same categories and genres. For those application the most common rating is \"0\". Apps with Everyone 10+ and is_not_equal_genre False  mostly have rating \"2\".  \n",
    "The most interesting thing is that the distribution of the rating for Teen and Mature 17+ applications  are fairly the same in general, they have a different distribution for applications with the same genre and category (*is_cat_equal_genre* True).\n",
    "\n",
    "<a id='category_&_price'></a>\n",
    "### 2.6. Category and Price\n",
    "\n",
    "Another convenient way to explore the data distribution is a *catplot*. In its simplest, it allows you to display each individual example in an individual column. This way you can get a sense of the data density. \n",
    "\n",
    "We will use the previously created logarithmic price column and consider only paid applications and categories with at least 10 such applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.catplot(x=\"Category\",y=\"log_price\", data=df_log[df_log.Type=='Paid'])\n",
    "g.fig.set_figheight(7)\n",
    "g.fig.set_figwidth(20)\n",
    "plt.title('The distribution of logarithmic price in different categories', size = 20)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are not a lot of paid applications, but numerous categories, so the graphics are so sparse. You can see that some categories have outlier applications: for instance, Communication category applications are mostly grouped together but one stands out from the others. Finance and Lifestyle are also quite sparse. Note, that the Family group contains a group of detached expensive applications.\n",
    "\n",
    "Now will take a look at the same distributions, but using confidence intervals.\n",
    ">**Confidence Intervals** - is a type of interval estimate used in statistics that are calculated for a given level of significance. They allow us to make a statement that the true value of an unknown statistical parameter of the general population is in the obtained range of values with a probability that is given by the selected level of statistical significance.\n",
    "\n",
    ">What is the practical meaning of the confidence interval?\n",
    "\n",
    ">+ A wide confidence interval indicates that the sample average doesn't accurately approximate the general average. This is usually related to an insufficient sample size, or to its heterogeneity, i.e. the large variance. Both give a large average error and, correspondingly, a wide confidence interval. This is the reason for returning to the planning stage of the study. \n",
    "\n",
    ">+ The upper and lower limits of the confidence interval give an estimation of whether the results will be statistically significant or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.catplot(x=\"Category\",y=\"log_price\", data=df_log[df_log.Type=='Paid'], kind='bar')\n",
    "g.fig.set_figheight(7)\n",
    "g.fig.set_figwidth(20)\n",
    "plt.title('The distribution of logarithmic price in different categories', size = 20)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this graph, the column height shows the average value, and the bar shows the confidence interval. You can notice that the Finance and Lifestyle categories have a very wide confidence interval. This means that these categories are highly sparse and have some isolated groups in different parts of the distributions. It is not valid to use the average value for such categories. There is no sense to consider other categories with a wide confidence interval - there are too few examples to interpret the stats meaningfully.\n",
    "\n",
    "The Family category has a narrow confidence interval: although there is a separate group of expensive applications, their number is small and the distribution of prices in this category is uniform in general.\n",
    "\n",
    "Let's look at the groups of expensive applications in the categories Finance and Lifestyle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_log.log_price[(df_log.Category.isin(['LIFESTYLE', 'FINANCE'])) & (df_log.Type == 'Paid')].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_log[(df_log.log_price > 5.) & (df_log.Category.isin(['LIFESTYLE', 'FINANCE']))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shift was due to a few expensive \"freaky\" apps we already saw earlier. Let's remove them and rearrange the graphic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.catplot(x=\"Category\",y=\"log_price\", data=df_log[(df_log.Type=='Paid') & (df_log.log_price < 5)], kind='bar')\n",
    "g.fig.set_figheight(7)\n",
    "g.fig.set_figwidth(20)\n",
    "plt.title('The distribution of logarithmic price in different categories', size = 20)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Lifestyle and Finance categories became more realistic and stable. So, we are removing extra expensive samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~((df_log.Category.isin(['LIFESTYLE', 'FINANCE'])) & (df_log.log_price > 5))]\n",
    "df_log = df_log[~((df_log.Category.isin(['LIFESTYLE', 'FINANCE'])) & (df_log.log_price > 5))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Категории_и_размер'></a>\n",
    "### 2.7. Category and Size\n",
    "\n",
    "Let's add the logarithm of size to df_log."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_log['log_size'] = df_log.Size.apply(np.log1p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 14** Explore the distribution of app size in each category. Use the df_log table and the log_size column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cedcdaba2883fa903962cb5e628db11a",
     "grade": false,
     "grade_id": "cell-6ab19be5f87fed9e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "##### Implement this part of the code #####\n",
    "raise NotImplementedError(\"Code not implemented, follow the instructions.\")\n",
    "# g = sns.catplot(x= ?, y= ?, data=df_log)\n",
    "\n",
    "g.fig.set_figheight(7)\n",
    "g.fig.set_figwidth(20)\n",
    "plt.title('The distribution of logarithmic size in different categories', size = 20)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3_feature_space'></a>\n",
    "## 3. Feature Engineering\n",
    "\n",
    "Let's create a table where we will store the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(columns=['method', 'model', 'val score', 'test score', 'learning time', 'predict time'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To measure processing time we will use a special context manager."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Timer(object):\n",
    "    def __init__(self):\n",
    "        self.elapsed_time = 0\n",
    "    def __enter__(self):\n",
    "        self.start = time()\n",
    "    def __exit__(self, type, value, traceback):\n",
    "        self.end = time()\n",
    "        self.elapsed_time = int((self.end - self.start)*1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timer = Timer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='base_model'></a>\n",
    "### 3.1. Base model\n",
    "\n",
    "Our task is to determine the rating of the application. Let's take this column into a separate variable, and remove it from the main dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = df.Rating.astype(int)\n",
    "df = df.drop(columns=['Rating'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The so-called *majority classifier* can be the simplest type of a base model. The point is: we always predict the class that is most common in our training dataset.\n",
    "\n",
    "Let's split the data into training and test sets. For appropriate comparision of the different models, this partition must always be the same, that's why we use fixed *random_state*. For now, we don't need the features, only the target variable.\n",
    "\n",
    "We remind you that the train/test split of dataset samples should actually be performed before data processing!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, y_test = train_test_split(Y,  test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's determine which class is most common in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes, counts = np.unique(y_train, return_counts=True)\n",
    "major_class = classes[np.argmax(counts)]\n",
    "major_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the training set, samples with second class are more common. Then our test prediction will be an array with the same size as the size of out test set and it will be completely filled with 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_predict = np.full(y_test.shape, major_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To analyze the results, we will use a function that displays several different metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(y_predict, y_test):\n",
    "    print('acсuracy: {:.4f}'.format(accuracy_score(y_predict, y_test)))\n",
    "    print('F1 score: {:.4f}'.format(f1_score(y_predict, y_test, average='macro')))\n",
    "    print(classification_report(y_predict, y_test))\n",
    "    print(confusion_matrix(y_predict, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's calculate the accuracy of the majority classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_metrics(base_predict, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These will be our reference values. Let's write them in the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = results.append({'method':'Baseline',\n",
    "                          'model': 'Majority',\n",
    "                          'val score': None,\n",
    "                          'test score': f1_score(base_predict, y_test, average='macro'), \n",
    "                          'learning time' : None,\n",
    "                          'predict time': None},\n",
    "                          ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='origin_features'></a>\n",
    "### 3.2. Basic features\n",
    "\n",
    "Now we are going to use the original feature space to predict the target variable and measure the accuracy of the prediction. We will use only numeric and boolean features: they don't require the preparation, unlike text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_df = df.select_dtypes(include=['int64', 'float64', 'bool'])\n",
    "numeric_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll get our test and training sets. (notice that we use the same random_state and therefore the data is split the same way as before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(numeric_df, Y, \n",
    "                                                    test_size=0.3, \n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will train a Logistic Regression model - one of the simplest ones among linear classifiers. To perform hyperparameter optimization we'll use cross-validation by applying Pipeline and GridSearchCV functions. This specific classifier was chosen in order to decrease the training time within the workshop. Results for more complex models will be provided in the bonus file.\n",
    "\n",
    "> **Data scaling** — the process of adjusting the data to a single scale. The main methods are **normalization** - adjusting all features to a value in the range from 0 to 1, and **standardization** - data preprocessing, after which each feature has an average of 0 and a variance of 1\n",
    "\n",
    "Scaling the data can significantly affect the process of training a model. For example, it increases the speed and the stability of gradient descent algorithm. Moreover, methods that rely on working in a multidimensional space (such as KNN) will not work correctly if the features have a different scale. These effects don't always appear, but scaling the data is unlikely to make the results worse, so you should always perform it before training the model.\n",
    "\n",
    "Now let's create a pipeline and set a grid of the hyperparameters for the classifier.\n",
    "\n",
    "Note: if the Logistic Regression algorithm does not converge because the of the parameters selected it will throw warnings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([('scale', StandardScaler()),\n",
    "                 ('clf', LogisticRegression(random_state=42))])\n",
    "\n",
    "params = {\n",
    "    'clf__C': [0.01, 0.05, 0.1, 0.5, 0.9, 0.99],\n",
    "    'clf__penalty': ['l1', 'l2', 'elasticnet']\n",
    "} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "\n",
    "clf = GridSearchCV(pipe, \n",
    "                   cv=3,  \n",
    "                   param_grid=params, \n",
    "                   scoring='f1_macro',\n",
    "                   verbose=1,\n",
    "                   n_jobs=6)\n",
    "\n",
    "with timer:\n",
    "    clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_time = timer.elapsed_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_clf = clf.best_estimator_.steps[1][1]\n",
    "best_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with timer:\n",
    "    predict = clf.predict(X_test)\n",
    "    \n",
    "predict_time = timer.elapsed_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_metrics(predict, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we shall save the results in the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = results.append({'method':'Numeric',\n",
    "                          'model': 'LR',\n",
    "                          'val score': clf.best_score_,\n",
    "                          'test score': f1_score(clf.predict(X_test), y_test, average='macro'), \n",
    "                          'learning time': learning_time,\n",
    "                          'predict time': predict_time},\n",
    "                          ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the F1-score of the model is slightly higher than the score of the majority classifier.\n",
    "\n",
    "As the next step we should add categorical features. First we will encode them features into numerical ones. This can be done in multiple ways.\n",
    "\n",
    "For example, you could take one column, index all unique category names with numbers and just replace them in the column. This type of encoding is called Label Encoding. This method is usually bad because most of the time the values of numbers don't reflect the category relation. For example, encoding the application with type GAME as 1 and with MEDICAL as 2 turns out that MEDICAL > GAME, while such a comparison is incorrect. Therefore, many methods, especially linear ones, will not work well with this type of encoding. At the same time, methods based on decision trees will work fine.\n",
    "\n",
    "Another popular type of encoding is called Dummy Encoding or One-hot Encoding. Each unique category name has its own column. The values in this column can only be 0 and 1. If the sample instance has some category, then the corresponding dummy-column will contain a \"1\" value, and all others will be filled with zeros. This type of encoding has the disadvantage that if there are a lot of unique categories, the size of the feature matrix becomes too large and sparse. Therefore additional preprocessing of categorical data may be necessary.\n",
    "\n",
    "There are other, less popular and well known techniques, but here we will stick to One-hot encoding.\n",
    "\n",
    "First of all, we need to copy all the data to the new dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dummies = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, it is necessary to delete the application names - they are unique for each sample. If we encode them with dummy encoding, it will lead to adding N, where N is the length of the whole dataset, and all these columns will have only one value 1 and all the others 0. Such features obviously do not work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dummies = df_dummies.drop(columns=['App'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As was said before, sometimes it makes sense to encode columns with LabelEncoding. In our case the Content Rating column is suitable for this approach. We should encode it in a way, so with the growth of restrictions, the corresponding number will also grow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con_rat_dict =  {'Everyone':0, 'Everyone 10+':1, 'Teen':2, 'Mature 17+':3}\n",
    "df_dummies = df_dummies.replace({\"Content Rating\": con_rat_dict})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's encode the remaining categories with the dummy method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dummies = pd.get_dummies(df_dummies)\n",
    "\n",
    "df_dummies.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have 44 features instead of 4.  \n",
    "Let's prepare a training and test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_dummies, Y,\n",
    "                                                    test_size=0.3, \n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 15** Train the model, make a prediction on the test data and output the statistics. Don't forget to measure your training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1a723ddb917635294903572f26c1c6f1",
     "grade": false,
     "grade_id": "cell-314a96a503b0f2ba",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "\n",
    "with timer:\n",
    "##### Implement this part of the code #####\n",
    "raise NotImplementedError(\"Code not implemented, follow the instructions.\")\n",
    "    # clf = GridSearchCV(pipe, cv=3,  \n",
    "                       # param_grid=params, \n",
    "                       # scoring='f1_macro', verbose=1,\n",
    "                       # n_jobs=6).fit(?, ?)\n",
    "    \n",
    "learning_time = timer.elapsed_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best score:', clf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with timer:\n",
    "    predict = clf.predict(X_test)\n",
    "    \n",
    "predict_time = timer.elapsed_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_metrics(predict, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = results.append({'method': 'Categories',\n",
    "                          'model': 'LR',\n",
    "                          'val score': clf.best_score_,\n",
    "                          'test score': f1_score(clf.predict(X_test), y_test, average='macro'), \n",
    "                          'learning time': learning_time,\n",
    "                          'predict time': predict_time},\n",
    "                          ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, adding categorical features have improved the accuracy of the model.\n",
    "\n",
    "<a id='feature_generation'></a>\n",
    "### 3.3. Feature generation\n",
    "\n",
    "Now we will expand our feature space by adding new features - manually crafted with love and care! \n",
    "\n",
    "First we need a separate copy of the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add a price per megabyte column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new['price_for_mb'] = df_new.Size/df_new.Price\n",
    "df_new['price_for_mb'] = df_new['price_for_mb'].replace([np.inf, -np.inf], 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also let's add logarithms for price and size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new['log_price'] = df_new.Price.apply(np.log1p)\n",
    "df_new['log_size'] = df_new.Size.apply(np.log1p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually it is useful to indroduce polynomial features for the numerical ones. Let's make them from the previously calculated logarithms of price and size. To do this, we can use the scikit-learn function Polynomial Features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = PolynomialFeatures(degree=2).fit(df_new[['log_price', 'log_size']])\n",
    "poly_df = pd.DataFrame(p.transform(df_new[['log_price', 'log_size']]), \n",
    "                        columns=p.get_feature_names(['log_price', 'log_size']))\n",
    "poly_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that there are new columns low_price^2, log_price, log_size and log_size^2. Now you need to attach them to the main dataset, and discard unnecessary ones. To avoid errors during concatenation, we will replace the poly_df index with the df_new index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_df = poly_df.drop(columns=['log_price', 'log_size', '1'])\n",
    "poly_df.index = df_new.index\n",
    "df_new = pd.concat([df_new, poly_df.reindex(df_new.index)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the number of characters and the number of words in the title as features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new['len_of_app_title'] = df_new.App.apply(len)\n",
    "df_new['count_of_app_title'] = df_new.App.str.split(' ').apply(len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The names of applications contain a lot of garbage. It is better to clean it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's change to lower case, remove special characters, leave only Latin letters and numbers.\n",
    "df_new['cleantext'] = df_new.App.str.lower()\n",
    "df_new['cleantext'] = df_new.cleantext.str.replace('[-_]', ' ')\n",
    "df_new['cleantext'] = df_new.cleantext.str.replace('[^0-9A-Za-z ]+', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to check what the text looks like after cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new[['App', 'cleantext']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, after filtering, we can add the number of characters and the number of words as features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new['len_of_cleantext_title'] = df_new.cleantext.apply(len)\n",
    "df_new['count_of_cleantext_title'] = df_new.cleantext.str.split(' ').apply(len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step is to add the difference between the original number of words and symbols and the normalized ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new['diff_len_title'] = df_new['len_of_app_title'] - df_new['len_of_cleantext_title']\n",
    "df_new['diff_count_title'] = df_new['count_of_app_title'] - df_new['count_of_cleantext_title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new[['diff_len_title', 'diff_count_title']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some reason there are normalized headers that have more words than original ones. We need to check it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new[['App', 'cleantext']][df_new['diff_count_title'] == -5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything seems fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_vocab = len(set(' '.join(df_new.cleantext.tolist()).split(' ')))\n",
    "max_count_of_cleantext_title = round(df_new.count_of_cleantext_title.max(), 0)\n",
    "print('Number of unique words: {}'.format(len_vocab))\n",
    "print('Maximum number of words in a title: {}'.format(max_count_of_cleantext_title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = df_new.drop(columns=['App', 'cleantext'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's encode categorical features into numerical vectors. \n",
    "\n",
    "**Task 16:** Encode categorical features to one-hot format. Don't forget about the special case with Content Rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a44e9c82185982fb126d3e75af629bce",
     "grade": false,
     "grade_id": "cell-b6291bc7ca15458e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "df_new = df_new.replace({\"Content Rating\": con_rat_dict})\n",
    "\n",
    "##### Implement this part of the code #####\n",
    "raise NotImplementedError(\"Code not implemented, follow the instructions.\")\n",
    "# df_new = pd. ?\n",
    "\n",
    "df_new.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to visualize our data. We will use UMAP for this.\n",
    "It is advisable to standardize the data before using this algorithm.\n",
    "\n",
    "\n",
    "> **UMAP** (Uniform Manifold Approximation and Projection) - this is a relatively new dimensionality reduction algorithm. Previously the most popular algorithm for these purposes was t-SNE. However UMAP is better in almost every way: UMAP has no restrictions on the dimension of the original feature space, it is much faster and more computationally efficient than t-SNE, and also better in the task of transferring the global data structure to a new, reduced space (as the authors of the algorithm claim). For these reasons t-SNE is hardly used these days.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_data = StandardScaler().fit_transform(df_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "np.random.seed(1)\n",
    "X_umap = umap.UMAP(n_components=2, random_state=2).fit_transform(scaled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[15, 9])\n",
    "plt.title('UMAP')\n",
    "\n",
    "for i in Y.unique():\n",
    "    indx = np.where(Y.values == i)\n",
    "    plt.scatter(X_umap[indx, 0], X_umap[indx, 1], marker='.', alpha=0.5, label=i)\n",
    "    plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that there is a lot of clusters and they contain different proportions of applications with different ratings. Let's apply a clustering algorithm to separate these groups. For now we will assume that we have 30 clusters and use the k_means algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls = cluster.KMeans(n_clusters=30, random_state=42)\n",
    "kmeans_labels = cls.fit_predict(scaled_data)\n",
    "\n",
    "# For Google Colab below\n",
    "\n",
    "# Unfortunately, Google collab doesn't allow to reproduce KMeans model training in spite of the random_state.\n",
    "# Without it we can't reproduce our clusters and distributions inside them. That's why we're loading pretrained cluster\n",
    "# model. If you want train your own cluster model you can use the commented code\n",
    "\n",
    "# m_file = BytesIO(requests.get(CLUSTER_F).content)\n",
    "# cls = load(m_file)\n",
    "# kmeans_labels = cls.predict(scaled_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we visualize the results of our clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[15, 9])\n",
    "ax = plt.subplot()\n",
    "\n",
    "for i in np.unique(kmeans_labels):\n",
    "    indx = np.where(kmeans_labels == i)\n",
    "    plt.scatter(X_umap[indx, 0], X_umap[indx, 1], marker='.', alpha=0.5, label=i)\n",
    "ax.legend(loc='upper center', bbox_to_anchor=(1.05, 1.0),  ncol=1, fancybox=True, shadow=True, fontsize=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks promissing. Let's explore whether there is a difference in the distributions of the target variable in each cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clstr = pd.DataFrame({'Cluster': kmeans_labels, 'Rating':Y})\n",
    "g = sns.catplot(\"Rating\", col=\"Cluster\", col_wrap=5, data=df_clstr, kind=\"count\", height=2.5, aspect=.8, size=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, the distribution of the target variable differs in many way throughout the clusters. We probably should add the cluster indexes to the main dataset. Note that the cluster index is a category. To perform one-hot encoding we will remove the target variable and then we can add new columns to the main dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clstr = df_clstr.drop(columns=['Rating'])\n",
    "df_clstr['Cluster'] = df_clstr['Cluster'].astype('category')\n",
    "df_clstr = pd.get_dummies(df_clstr)\n",
    "df_clstr.index = df_new.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new=pd.concat([df_new, df_clstr], axis=1)\n",
    "df_new.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It also would be possible to use aggregation features: for example, average, median, minimum, etc. for each category, age rating or other categorical features. You can apply simple mathematical operations to pairs of numerical features (for example, multiply the size of the application by the price): sometimes they don't make explicit sense, but they might be useful. There are also cases of logical addition of features (there is a category Medicine and an age rating of 18+, for example). To obtain such features, it is convenient to use the *featuretools* library. In general, the number of possible new features is just limited only by your imagination. Unfortunately today we don't have enough time go all crazy with manually creating these feature, so we will limit ourselves to the ones that we have already discussed.\n",
    "\n",
    "Now let's try see if our work paid off\n",
    "\n",
    "**Task 17:** Split the dataset and target variable into training and test set, train the model, make a prediction on the test data and output the statistics. Don't forget to measure your training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_new, Y,\n",
    "                                                    test_size=0.3, \n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8cdd3e1b9e93b7f81916354893667cb3",
     "grade": false,
     "grade_id": "cell-55a165066d62d611",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "\n",
    "##### Implement this part of the code #####\n",
    "raise NotImplementedError(\"Code not implemented, follow the instructions.\")\n",
    "# with ?:\n",
    "#    clf = GridSearchCV( ? ).fit( ? )\n",
    "\n",
    "learning_time = timer.elapsed_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best score:', clf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best params:', clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with timer:\n",
    "    predict = clf.predict(X_test)\n",
    "    \n",
    "predict_time = timer.elapsed_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_metrics(predict, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = results.append({'method': 'Manual features',\n",
    "                          'model': 'LR',\n",
    "                          'val score': clf.best_score_,\n",
    "                          'test score': f1_score(clf.predict(X_test), y_test, average='macro'), \n",
    "                          'learning time':learning_time,\n",
    "                          'predict time': predict_time},\n",
    "                          ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding the manually created features increased the effectiveness of the model.\n",
    "\n",
    "<a id='dimensional_reduction'></a>\n",
    "## 4. Feature Selection and Dimensionality Reduction\n",
    "\n",
    "Usualy it is useful to know which feature are actually doing the work, so we can drop all others as they can actually cause over-fitting and thus hurt the final score. In our case there are only 123 features, but it can be tens and hundreds of thousands. The goal is to reduce the dimensionality of the feature space and not lose useful information.\n",
    "\n",
    "There are various ways to reduce the dimension of the data:\n",
    ">*Statistical methods of dimension reduction*\n",
    "1. Principal Component Analysis\n",
    "2. Linear Discriminant Analysis\n",
    "3. Factor analysis\n",
    "\n",
    ">*Machine Learning based algorithms\n",
    "1. T-SNE \n",
    "2. UMAP and others.\n",
    "\n",
    ">*Methods based on feature selection*\n",
    "1. The significance of the features (determined by the training of some models)\n",
    "2. Filtering methods (rank features based on statistic functions - e.g. Chi square test, ANOVA)\n",
    "3. Lasso and Ridge regression\n",
    "4. Greedy algorithm - step by step adding features (Forward Feature Selection) or Backward Feature Elimination\n",
    "\n",
    ">Autoencoders should be noted as a method of reducing the dimension of data but will not be covered here. \n",
    "\n",
    "We will look at a couple of these methods: Principal Component Analysis and Backward Feature Elimination.\n",
    "\n",
    "<a id='pca'></a>\n",
    "### 4.1. Principal Component Analysis\n",
    "\n",
    "Principal Component Analysis (PCA) - one of the popular ways to reduce data dimensionality with minimum loss of information. The challenge of principal component analysis is to find subspaces of smaller dimension, in orthogonal projection on which the data variation (that is, the standard deviation from the mean value) is maximal.\n",
    "\n",
    "In simple terms, the method finds the n-th number of new axes, projections on which retain as much variance of the original data as possible. The projections themselves become new features.\n",
    " \n",
    "It should be used carefully, because the feature space is radically changing. The information necessary for the algorithms performance can be lost even with a high retention of information content. Also it is very prone to overfitting.\n",
    "\n",
    "Since the method is related to variance of features, it is necessary to scale data before using PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PCA()\n",
    "pca_data = model.fit_transform(StandardScaler().fit_transform(df_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[13, 5])\n",
    "plt.title('Principal Component Analysis (PCA)')\n",
    "plt.plot(range(len(model.explained_variance_ratio_)), model.explained_variance_ratio_, '--o')\n",
    "plt.ylabel('Explained variance ratio')\n",
    "plt.xlabel('Components')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apparently, most of the information can be saved with 60-70 new projections. Let's see how many components do we need to keep 99% of the original dataset's information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 18** Apply the PCA method to the *df_new* dataset again, but set the *n_components* parameter to 0.99. If this parameter is an integer, then in the final matrix will be left such a number of features. If it is the float value from 0 to 1, the features will be selected automatically in order to save this amount of information. Print the size of the resulting matrix  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a49f420d13372c943479e53d79ed5d88",
     "grade": false,
     "grade_id": "cell-9ad92d3fb1fdd310",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "##### Implement this part of the code #####\n",
    "raise NotImplementedError(\"Code not implemented, follow the instructions.\")\n",
    "# model = PCA(n_components=?)\n",
    "# pca_data = model.fit_transform(?)\n",
    "\n",
    "assert pca_data.shape[1] == 48\n",
    "pca_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='greedy_selection'></a>\n",
    "### 4.2. Greedy selection\n",
    "\n",
    "Now we will use the most reliable, but very resource-intensive method - iterative brute force feature selection. We will remove one feature at a time, predict the target variable and see how the quality metric changes: the more the metric decreases after removing the feature, the more important this feature is. More than that, sometimes the metric might even improve.\n",
    "\n",
    "This method takes a lot of time. Therefore, we won't calculate it now, but you can see the code and its results below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(df_new, Y,\n",
    "#                                                     test_size=0.3,\n",
    "#                                                     random_state=42)\n",
    "\n",
    "# scaled_data = StandardScaler().fit_transform(X_train)\n",
    "# scaled_X = pd.DataFrame(scaled_data, index=X_train.index, columns=X_train.columns)\n",
    "\n",
    "# clf_logreg = LogisticRegression(random_state=42, C=0.05, penalty='l2')\n",
    "\n",
    "# parameters_grid = {'C': [0.05]}\n",
    "# columns = X_train.columns\n",
    "# important_features = []\n",
    "# features_scores = []\n",
    "\n",
    "\n",
    "# np.random.seed(123)\n",
    "\n",
    "\n",
    "# for j in range(len(columns)-1):\n",
    "#     print('{}\\{}'.format(j, len(columns)))\n",
    "#     col_for_del = []\n",
    "#     scores = []\n",
    "#     for i in columns:\n",
    "#         cols = columns[columns != i]\n",
    "#         clf = GridSearchCV(clf_logreg, cv=3,\n",
    "#                            param_grid=parameters_grid,\n",
    "#                            scoring='f1_macro',\n",
    "#                            verbose=0,\n",
    "#                            n_jobs=6).fit(scaled_X[cols], y_train)\n",
    "\n",
    "#         scores.append(clf.best_score_)\n",
    "#     max_col = columns[np.argmax(scores)]\n",
    "#     print(max_col, clf.best_score_)\n",
    "#     important_features.append(max_col)\n",
    "#     features_scores.append(max(scores))\n",
    "#     columns = columns[columns != max_col]\n",
    "#     print(j, '\\r', end='')\n",
    "# features_scores.append(0)\n",
    "# important_features.append(columns[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_importances = pd.DataFrame({'features': important_features, \n",
    "#                                     'feature_importances': features_scores,\n",
    "#                                     'iteration': range(len(features_scores))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_importances = feature_importances.sort_values('iteration', ascending=True)\n",
    "# feature_importances.to_csv(FEATURE_IMPORTANCE_F, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = pd.read_csv(FEATURE_IMPORTANCE_F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[15, 6])\n",
    "plt.title('Brute force')\n",
    "plt.plot(feature_importances.iteration[:-1], \n",
    "         feature_importances.feature_importances[:-1], '-o')\n",
    "plt.ylabel('f1_macro')\n",
    "plt.xlabel('iteration')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top 20 of the most significant features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances.features[::-1][:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find at what stage was the best accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances[feature_importances.feature_importances == feature_importances.feature_importances.max()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_id = feature_importances[\n",
    "    feature_importances.feature_importances == feature_importances.feature_importances.max()\n",
    "].index[0]\n",
    "\n",
    "print('We can leave {} features'.format(len(feature_importances)-max_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = feature_importances.features[max_id:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='4_final_prediction'></a>\n",
    "### 4.3. Final prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 19** Train the model on the features obtained by the greedy selection method and the PCA method. Add the results to the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8337e9a7c4d00079c3d3a664d999bffa",
     "grade": false,
     "grade_id": "cell-b507079913f283ae",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Greedy features\n",
    "\n",
    "##### Implement this part of the code #####\n",
    "raise NotImplementedError(\"Code not implemented, follow the instructions.\")\n",
    "# X_train, X_test, y_train, y_test = train_test_split(df_new[ ? ], Y,                                                   \n",
    "#                                                     test_size=0.3, \n",
    "#                                                     random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cd5b5db410c16519b71ff75d7917bef3",
     "grade": false,
     "grade_id": "cell-02d6598c6d0ec9ec",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "pipe = Pipeline([('scale', StandardScaler()),\n",
    "                 ('clf', LogisticRegression(random_state=42))])\n",
    "\n",
    "params = {\n",
    "    'clf__C': [0.01, 0.05, 0.1, 0.5, 0.9, 0.99],\n",
    "    'clf__penalty': ['l1', 'l2', 'elasticnet']\n",
    "} \n",
    "\n",
    "np.random.seed(123)\n",
    "\n",
    "##### Implement this part of the code #####\n",
    "raise NotImplementedError(\"Code not implemented, follow the instructions.\")\n",
    "# with ? :\n",
    "#     clf = GridSearchCV( ? ).fit( ? )\n",
    "    \n",
    "learning_time = timer.elapsed_time\n",
    "\n",
    "with timer:\n",
    "    predict = clf.predict(X_test)\n",
    "    \n",
    "predict_time = timer.elapsed_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_metrics(predict, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = results.append({'method': 'Greedy selection',\n",
    "                          'model': 'LR',\n",
    "                          'val score': clf.best_score_,\n",
    "                          'test score': f1_score(clf.predict(X_test), y_test, average='macro'), \n",
    "                          'learning time': learning_time,\n",
    "                          'predict time': predict_time},\n",
    "                          ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1d0aa3f8d9f817d98242e45b1ad53bbd",
     "grade": false,
     "grade_id": "cell-815cae5b7e46869d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# PCA features\n",
    "\n",
    "##### Implement this part of the code #####\n",
    "raise NotImplementedError(\"Code not implemented, follow the instructions.\")\n",
    "# X_train, X_test, y_train, y_test = train_test_split( ? )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5fac81a1c44ab5a78130182ddb7f4d3d",
     "grade": false,
     "grade_id": "cell-8a84c919afdba207",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "\n",
    "pipe = Pipeline([('scale', StandardScaler()),\n",
    "                 ('pca', PCA(n_components=0.99)),\n",
    "                 ('clf', LogisticRegression(random_state=42))])\n",
    "\n",
    "##### Implement this part of the code #####\n",
    "raise NotImplementedError(\"Code not implemented, follow the instructions.\")\n",
    "# with ?\n",
    "#     clf = ?    \n",
    "\n",
    "learning_time = timer.elapsed_time\n",
    "\n",
    "with timer:\n",
    "    predict = clf.predict(X_test)\n",
    "    \n",
    "predict_time = timer.elapsed_time    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_metrics(clf.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = results.append({'method': 'PCA features',\n",
    "                          'model': 'LR',\n",
    "                          'val score': clf.best_score_,\n",
    "                          'test score': f1_score(clf.predict(X_test), y_test, average='macro'), \n",
    "                          'learning time': learning_time,\n",
    "                          'predict time': predict_time},\n",
    "                          ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='conclusions'></a>\n",
    "### 4.4. Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best result on the validation set was shown by a model trained on the features obtained by the greedy selection method. Because some features can decrease the accuracy, removing them allows you to get a better result than on the original dataset. However, on the test set, the greedy algorithm showed slightly worse results than the full dataset. This could happen due to the fact that the optimal set of features was selected for the validation set which in our case might have a slightly different distribution and the dataset isn't particularly large. \n",
    "\n",
    "The model trained on the features obtained with the PCA method provides an accuracy comparable to the results provided by the models trained on the raw data, but at the same time it learns much faster and the difference between val and test scores is negligible.\n",
    "\n",
    "We can't say that some approach is definitely better than others, it depends on the task. In some cases, greedy selection can significantly improve the result, but the time it takes can be extremely long and it can overfit. Using the principal component analysis allows you quickly and easily reduce the dimensionality, which can drastically speed up the training and prediction processes, however there may be a loss in accuracy. There are a lot of other algorithms to perform these actions, from this point it's for you to decide which result is more suitable for you.\n",
    "\n",
    "<a id='bonus'></a>\n",
    "## 5. Bonus\n",
    "\n",
    "Below you can find an example of comparative training of different models on different sets of features with a wide random selection of hyperparameters. Training takes a lot of time, so we have commented the code. You can load and view the results at the very end of the script. These results were obtained on a slightly different set of features, so they may differ from the results obtained in the workshop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from catboost import CatBoostClassifier\n",
    "# from lightgbm import LGBMClassifier\n",
    "\n",
    "# log_params = {'clf__C': uniform(0.01, 0.99),\n",
    "#               'clf__penalty': ['l1', 'l2']} \n",
    "\n",
    "# rf_params = {'clf__n_estimators': randint(100, 1000),\n",
    "#              'clf__max_depth': randint(1,10),\n",
    "#              'clf__min_samples_leaf': randint(1, 10),\n",
    "#              'clf__min_samples_split': randint(2, 11)} \n",
    "\n",
    "# gb_params = {'clf__learning_rate': uniform(0.01, 0.49),\n",
    "#              'clf__n_estimators': randint(100, 700),\n",
    "#              'clf__max_depth': randint(1, 10),\n",
    "#              'clf__subsample': uniform(0.6, 0.4)}   \n",
    "\n",
    "# models_dict = {'Logistic Regression':[log_params, LogisticRegression(random_state=42)],\n",
    "#                'Random Forest':[rf_params, RandomForestClassifier(random_state=42)],\n",
    "#                'Gradient Boosting': [gb_params, GradientBoostingClassifier(random_state=42)],\n",
    "#                'XGB':[gb_params, xgb.XGBClassifier(random_state=42)],\n",
    "#                'CatBoost':[gb_params, CatBoostClassifier(bootstrap_type='Bernoulli', random_state=42, verbose=0)],\n",
    "#                'LightBoost':[gb_params, LGBMClassifier(random_state=42)]}\n",
    "\n",
    "# ext_results = pd.DataFrame(columns=['method', 'model', 'val score', 'test score', 'learning time', 'predict time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def custom_pipe(clf, pca='False'):\n",
    "#     if pca:\n",
    "#         return  Pipeline([('clf', clf)])\n",
    "#     else:\n",
    "#         return Pipeline([('scale', StandardScaler()),\n",
    "#                          ('clf', clf)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_gs, X_test_gs = train_test_split(df_new[selected_features],test_size=0.3, random_state=42)\n",
    "# X_train_all, X_test_all = train_test_split(df_new,test_size=0.3, random_state=42)\n",
    "\n",
    "# pca_model = PCA(n_components=0.99)\n",
    "# scaler = StandardScaler()\n",
    "# X_train_pca = pca_model.fit_transform(scaler.fit_transform(X_train_all))\n",
    "# X_test_pca = pca_model.transform(scaler.transform(X_test_all))\n",
    "\n",
    "# data_dict = {'Manual features':(X_train_all, X_test_all),  \n",
    "#              'Greedy selection':(X_train_gs, X_test_gs),\n",
    "#              'PCA':(X_train_pca, X_test_pca)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.seed(123)\n",
    "# for method, (X_train, X_test) in data_dict.items():\n",
    "#     print(f'Method: {method}\\n')\n",
    "    \n",
    "#     if method == \"PCA\":\n",
    "#         pca_flag = True\n",
    "#     else:\n",
    "#         pca_flag = False\n",
    "    \n",
    "#     for clf_name, vals in models_dict.items():\n",
    "#         print(clf_name, '\\n')\n",
    "\n",
    "#         pipe = custom_pipe(vals[1], pca_flag)\n",
    "        \n",
    "#         with timer:\n",
    "#             clf = RandomizedSearchCV(pipe, cv=3, random_state=123, \n",
    "#                                      param_distributions=vals[0], \n",
    "#                                      n_jobs=4,\n",
    "#                                      verbose=1,\n",
    "#                                      n_iter=100,\n",
    "#                                      scoring='f1_macro').fit(X_train, y_train)\n",
    "\n",
    "#         print(clf.best_params_ , '\\n')\n",
    "\n",
    "#         learning_time = int(timer.elapsed_time / 300)\n",
    "\n",
    "#         with timer:\n",
    "#             predict = clf.predict(X_test)\n",
    "\n",
    "#         predict_time = timer.elapsed_time\n",
    "\n",
    "#         ext_results = ext_results.append({'method': method,\n",
    "#                                           'model': clf_name,\n",
    "#                                           'val score': clf.best_score_,\n",
    "#                                           'test score': f1_score(predict, y_test, average='macro'), \n",
    "#                                           'learning time': learning_time,\n",
    "#                                           'predict time': predict_time},\n",
    "#                                           ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ext_results.to_csv(BONUS_F, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ext_results = pd.read_csv(BONUS_F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ext_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training was carried out with a wide choice of parameters. The best result on the validation set was shown by CatBoost on all the features. It also showed the best result on the test set, but on the features obtained with greedy selection. In general, models based on gradient boosting show a similar results. Random Forest is a little behind them. Logistic Regression gives the weakest results.\n",
    "\n",
    "\n",
    "The accuracy of the models trained on the features obtained with the PCA is lower than in other cases. This is especially evident in the test sample. At the same time, training on them wasn't faster than on other methods, and sometimes even more. \n",
    "\n",
    "The comparison of learning time of gradient boosting models with RandomizedSearchCV is not absolutely accurate, as it depends on the *n_estimators* hyperparameter and it is different for each case. \n",
    "\n",
    "But in general, it can be noted that CatBoost is trained much longer than other gradient methods. However, it predicts very fast. LightBoost is the fastest learning gradient boosting model. In general, the fastest model in learning and prediction is a logistic regression. So if time is more valuable than accuracy, linear methods are the best choice. If you still want a sufficient accuracy, you should use LightBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
